{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - Syft Duet - Data Scientist ü•Å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "unconfined": true,
       "width": 400
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§  üé∏  ‚ô™‚ô™‚ô™ Joining Duet ‚ô´‚ô´‚ô´  üéª  üéπ\n",
      "\n",
      "‚ô´‚ô´‚ô´ >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "‚ô´‚ô´‚ô´ > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > ‚ù§Ô∏è \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "‚ô´‚ô´‚ô´ > Punching through firewall to OpenGrid Network Node at:\n",
      "‚ô´‚ô´‚ô´ > http://ec2-18-216-8-163.us-east-2.compute.amazonaws.com:5000\n",
      "‚ô´‚ô´‚ô´ >\n",
      "‚ô´‚ô´‚ô´ > ...waiting for response from OpenGrid Network... \n",
      "‚ô´‚ô´‚ô´ > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "‚ô´‚ô´‚ô´ > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 0 : Now STOP and run the Data Owner notebook until the next checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Setting up a Model and our Data\n",
    "The majority of the code below has been adapted closely from the original PyTorch MNIST example which is available in the `original` directory with these notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `duet` variable is now your reference to a whole world of remote operations including supported libraries like torch.\n",
    "\n",
    "Lets take a look at the duet.torch attribute.\n",
    "```\n",
    "duet.torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module:\n",
       "\t.Tensor -> <syft.ast.klass.Class object at 0x12fbd4160>\n",
       "\t.BFloat16Tensor -> <syft.ast.klass.Class object at 0x12fbd41c0>\n",
       "\t.BoolTensor -> <syft.ast.klass.Class object at 0x12fbd4220>\n",
       "\t.ByteTensor -> <syft.ast.klass.Class object at 0x12fbd4280>\n",
       "\t.CharTensor -> <syft.ast.klass.Class object at 0x12fbd42e0>\n",
       "\t.DoubleTensor -> <syft.ast.klass.Class object at 0x12fbd4340>\n",
       "\t.FloatTensor -> <syft.ast.klass.Class object at 0x12fbd43a0>\n",
       "\t.HalfTensor -> <syft.ast.klass.Class object at 0x12fbd4400>\n",
       "\t.IntTensor -> <syft.ast.klass.Class object at 0x12fbd4460>\n",
       "\t.LongTensor -> <syft.ast.klass.Class object at 0x12fbd44c0>\n",
       "\t.ShortTensor -> <syft.ast.klass.Class object at 0x12fbd4520>\n",
       "\t.nn -> Module:\n",
       "\t\t.Parameter -> <syft.ast.klass.Class object at 0x12fbd4640>\n",
       "\t\t.Module -> <syft.ast.klass.Class object at 0x12fc98940>\n",
       "\t\t.Conv2d -> <syft.ast.klass.Class object at 0x12fc98d00>\n",
       "\t\t.Dropout2d -> <syft.ast.klass.Class object at 0x12fc9d0a0>\n",
       "\t\t.Linear -> <syft.ast.klass.Class object at 0x12fc9d340>\n",
       "\t\t.functional -> Module:\n",
       "\t\t\t.relu -> <syft.ast.callable.Callable object at 0x12fc9db80>\n",
       "\t\t\t.max_pool2d -> <syft.ast.callable.Callable object at 0x12fc9dbe0>\n",
       "\t\t\t.log_softmax -> <syft.ast.callable.Callable object at 0x12fc9dc40>\n",
       "\t\t\t.cosine_embedding_loss -> <syft.ast.callable.Callable object at 0x12fca7940>\n",
       "\t\t\t.ctc_loss -> <syft.ast.callable.Callable object at 0x12fca79a0>\n",
       "\t\t\t.hinge_embedding_loss -> <syft.ast.callable.Callable object at 0x12fca7a60>\n",
       "\t\t\t.l1_loss -> <syft.ast.callable.Callable object at 0x12fca7ac0>\n",
       "\t\t\t.margin_ranking_loss -> <syft.ast.callable.Callable object at 0x12fca7b80>\n",
       "\t\t\t.mse_loss -> <syft.ast.callable.Callable object at 0x12fca7be0>\n",
       "\t\t\t.multi_margin_loss -> <syft.ast.callable.Callable object at 0x12fca7ca0>\n",
       "\t\t\t.multilabel_margin_loss -> <syft.ast.callable.Callable object at 0x12fca7d60>\n",
       "\t\t\t.multilabel_soft_margin_loss -> <syft.ast.callable.Callable object at 0x12fca7e20>\n",
       "\t\t\t.nll_loss -> <syft.ast.callable.Callable object at 0x12fca7e80>\n",
       "\t\t\t.poisson_nll_loss -> <syft.ast.callable.Callable object at 0x12fca7f40>\n",
       "\t\t\t.smooth_l1_loss -> <syft.ast.callable.Callable object at 0x12fcab040>\n",
       "\t\t\t.soft_margin_loss -> <syft.ast.callable.Callable object at 0x12fcab100>\n",
       "\t\t\t.triplet_margin_loss -> <syft.ast.callable.Callable object at 0x12fcab1c0>\n",
       "\n",
       "\t\t.Sequential -> <syft.ast.klass.Class object at 0x12fca7640>\n",
       "\t\t.AdaptiveLogSoftmaxWithLoss -> <syft.ast.klass.Class object at 0x12fcab280>\n",
       "\t\t.BCELoss -> <syft.ast.klass.Class object at 0x12fcab400>\n",
       "\t\t.BCEWithLogitsLoss -> <syft.ast.klass.Class object at 0x12fcab4c0>\n",
       "\t\t.CTCLoss -> <syft.ast.klass.Class object at 0x12fcab5e0>\n",
       "\t\t.CrossEntropyLoss -> <syft.ast.klass.Class object at 0x12fcab6a0>\n",
       "\t\t.CosineEmbeddingLoss -> <syft.ast.klass.Class object at 0x12fcab7c0>\n",
       "\t\t.HingeEmbeddingLoss -> <syft.ast.klass.Class object at 0x12fcab8e0>\n",
       "\t\t.KLDivLoss -> <syft.ast.klass.Class object at 0x12fcaba00>\n",
       "\t\t.L1Loss -> <syft.ast.klass.Class object at 0x12fcabac0>\n",
       "\t\t.MSELoss -> <syft.ast.klass.Class object at 0x12fcabb80>\n",
       "\t\t.MarginRankingLoss -> <syft.ast.klass.Class object at 0x12fcabc40>\n",
       "\t\t.MultiLabelMarginLoss -> <syft.ast.klass.Class object at 0x12fcabd60>\n",
       "\t\t.MultiLabelSoftMarginLoss -> <syft.ast.klass.Class object at 0x12fcabee0>\n",
       "\t\t.MultiMarginLoss -> <syft.ast.klass.Class object at 0x12fcb0040>\n",
       "\t\t.NLLLoss -> <syft.ast.klass.Class object at 0x12fcb0160>\n",
       "\t\t.NLLLoss2d -> <syft.ast.klass.Class object at 0x12fcb0220>\n",
       "\t\t.PoissonNLLLoss -> <syft.ast.klass.Class object at 0x12fcb02e0>\n",
       "\t\t.SmoothL1Loss -> <syft.ast.klass.Class object at 0x12fcb0400>\n",
       "\t\t.SoftMarginLoss -> <syft.ast.klass.Class object at 0x12fcb04c0>\n",
       "\t\t.TripletMarginLoss -> <syft.ast.klass.Class object at 0x12fcb05e0>\n",
       "\t\t.AdaptiveAvgPool1d -> <syft.ast.klass.Class object at 0x12fcb0700>\n",
       "\t\t.AdaptiveAvgPool2d -> <syft.ast.klass.Class object at 0x12fcb0ca0>\n",
       "\t\t.AdaptiveAvgPool3d -> <syft.ast.klass.Class object at 0x12fcb4280>\n",
       "\t\t.AdaptiveMaxPool1d -> <syft.ast.klass.Class object at 0x12fcb4820>\n",
       "\t\t.AdaptiveMaxPool2d -> <syft.ast.klass.Class object at 0x12fcb4dc0>\n",
       "\t\t.AdaptiveMaxPool3d -> <syft.ast.klass.Class object at 0x12fcb63a0>\n",
       "\t\t.AlphaDropout -> <syft.ast.klass.Class object at 0x12fcb6940>\n",
       "\t\t.AvgPool1d -> <syft.ast.klass.Class object at 0x12fcb6e20>\n",
       "\t\t.AvgPool2d -> <syft.ast.klass.Class object at 0x12fcb9220>\n",
       "\t\t.AvgPool3d -> <syft.ast.klass.Class object at 0x12fcb95e0>\n",
       "\t\t.BatchNorm1d -> <syft.ast.klass.Class object at 0x12fcb99a0>\n",
       "\t\t.BatchNorm2d -> <syft.ast.klass.Class object at 0x12fcb9d60>\n",
       "\t\t.BatchNorm3d -> <syft.ast.klass.Class object at 0x12fcbf160>\n",
       "\t\t.Bilinear -> <syft.ast.klass.Class object at 0x12fcbf520>\n",
       "\t\t.CELU -> <syft.ast.klass.Class object at 0x12fcbf8e0>\n",
       "\t\t.ConstantPad1d -> <syft.ast.klass.Class object at 0x12fcbfc40>\n",
       "\t\t.ConstantPad2d -> <syft.ast.klass.Class object at 0x12fcc2160>\n",
       "\t\t.ConstantPad3d -> <syft.ast.klass.Class object at 0x12fcc2640>\n",
       "\t\t.Container -> <syft.ast.klass.Class object at 0x12fcc2b20>\n",
       "\t\t.Conv1d -> <syft.ast.klass.Class object at 0x12fcc2ee0>\n",
       "\t\t.Conv3d -> <syft.ast.klass.Class object at 0x12fcc6280>\n",
       "\t\t.ConvTranspose1d -> <syft.ast.klass.Class object at 0x12fcc65e0>\n",
       "\t\t.ConvTranspose2d -> <syft.ast.klass.Class object at 0x12fcc6b20>\n",
       "\t\t.ConvTranspose3d -> <syft.ast.klass.Class object at 0x12fccb0a0>\n",
       "\t\t.CosineSimilarity -> <syft.ast.klass.Class object at 0x12fccb5e0>\n",
       "\t\t.CrossMapLRN2d -> <syft.ast.klass.Class object at 0x12fccbb20>\n",
       "\t\t.DataParallel -> <syft.ast.klass.Class object at 0x12fccf040>\n",
       "\t\t.Dropout -> <syft.ast.klass.Class object at 0x12fccf520>\n",
       "\t\t.Dropout3d -> <syft.ast.klass.Class object at 0x12fccfa00>\n",
       "\t\t.ELU -> <syft.ast.klass.Class object at 0x12fccfdc0>\n",
       "\t\t.Embedding -> <syft.ast.klass.Class object at 0x12fcd1160>\n",
       "\t\t.EmbeddingBag -> <syft.ast.klass.Class object at 0x12fcd1520>\n",
       "\t\t.FeatureAlphaDropout -> <syft.ast.klass.Class object at 0x12fcd1a00>\n",
       "\t\t.Flatten -> <syft.ast.klass.Class object at 0x12fcd60a0>\n",
       "\t\t.Fold -> <syft.ast.klass.Class object at 0x12fcd6460>\n",
       "\t\t.FractionalMaxPool2d -> <syft.ast.klass.Class object at 0x12fcd67c0>\n",
       "\t\t.FractionalMaxPool3d -> <syft.ast.klass.Class object at 0x12fcd6e20>\n",
       "\t\t.GELU -> <syft.ast.klass.Class object at 0x12fcd94c0>\n",
       "\t\t.GLU -> <syft.ast.klass.Class object at 0x12fcd9820>\n",
       "\t\t.GRU -> <syft.ast.klass.Class object at 0x12fcd9b80>\n",
       "\t\t.GRUCell -> <syft.ast.klass.Class object at 0x12fcd9ee0>\n",
       "\t\t.GroupNorm -> <syft.ast.klass.Class object at 0x12fcdd2e0>\n",
       "\t\t.Hardshrink -> <syft.ast.klass.Class object at 0x12fcdd6a0>\n",
       "\t\t.Hardsigmoid -> <syft.ast.klass.Class object at 0x12fcdda60>\n",
       "\t\t.Hardswish -> <syft.ast.klass.Class object at 0x12fcdde20>\n",
       "\t\t.Hardtanh -> <syft.ast.klass.Class object at 0x12fce2220>\n",
       "\t\t.Identity -> <syft.ast.klass.Class object at 0x12fce25e0>\n",
       "\t\t.InstanceNorm1d -> <syft.ast.klass.Class object at 0x12fce29a0>\n",
       "\t\t.InstanceNorm2d -> <syft.ast.klass.Class object at 0x12fce2ee0>\n",
       "\t\t.InstanceNorm3d -> <syft.ast.klass.Class object at 0x12fce7460>\n",
       "\t\t.LPPool1d -> <syft.ast.klass.Class object at 0x12fce79a0>\n",
       "\t\t.LPPool2d -> <syft.ast.klass.Class object at 0x12fce7d60>\n",
       "\t\t.LSTM -> <syft.ast.klass.Class object at 0x12fcea160>\n",
       "\t\t.LSTMCell -> <syft.ast.klass.Class object at 0x12fcea4c0>\n",
       "\t\t.LayerNorm -> <syft.ast.klass.Class object at 0x12fcea880>\n",
       "\t\t.LeakyReLU -> <syft.ast.klass.Class object at 0x12fceac40>\n",
       "\t\t.LocalResponseNorm -> <syft.ast.klass.Class object at 0x12fcee040>\n",
       "\t\t.LogSigmoid -> <syft.ast.klass.Class object at 0x12fcee5e0>\n",
       "\t\t.LogSoftmax -> <syft.ast.klass.Class object at 0x12fcee9a0>\n",
       "\t\t.MaxPool1d -> <syft.ast.klass.Class object at 0x12fceed60>\n",
       "\t\t.MaxPool2d -> <syft.ast.klass.Class object at 0x12fcf2160>\n",
       "\t\t.MaxPool3d -> <syft.ast.klass.Class object at 0x12fcf2520>\n",
       "\t\t.MaxUnpool1d -> <syft.ast.klass.Class object at 0x12fcf28e0>\n",
       "\t\t.MaxUnpool2d -> <syft.ast.klass.Class object at 0x12fcf2ca0>\n",
       "\t\t.MaxUnpool3d -> <syft.ast.klass.Class object at 0x12fcf70a0>\n",
       "\t\t.ModuleDict -> <syft.ast.klass.Class object at 0x12fcf7460>\n",
       "\t\t.ModuleList -> <syft.ast.klass.Class object at 0x12fcf7820>\n",
       "\t\t.MultiheadAttention -> <syft.ast.klass.Class object at 0x12fcf7be0>\n",
       "\t\t.PReLU -> <syft.ast.klass.Class object at 0x12fcfb220>\n",
       "\t\t.PairwiseDistance -> <syft.ast.klass.Class object at 0x12fcfb580>\n",
       "\t\t.PixelShuffle -> <syft.ast.klass.Class object at 0x12fcfbac0>\n",
       "\t\t.RNN -> <syft.ast.klass.Class object at 0x12fcfbfa0>\n",
       "\t\t.RNNBase -> <syft.ast.klass.Class object at 0x12fcfe340>\n",
       "\t\t.RNNCell -> <syft.ast.klass.Class object at 0x12fcfe700>\n",
       "\t\t.RNNCellBase -> <syft.ast.klass.Class object at 0x12fcfeac0>\n",
       "\t\t.RReLU -> <syft.ast.klass.Class object at 0x12fcfee80>\n",
       "\t\t.ReLU -> <syft.ast.klass.Class object at 0x12fd04220>\n",
       "\t\t.ReLU6 -> <syft.ast.klass.Class object at 0x12fd04580>\n",
       "\t\t.ReflectionPad1d -> <syft.ast.klass.Class object at 0x12fd048e0>\n",
       "\t\t.ReflectionPad2d -> <syft.ast.klass.Class object at 0x12fd04e20>\n",
       "\t\t.ReplicationPad1d -> <syft.ast.klass.Class object at 0x12fd083a0>\n",
       "\t\t.ReplicationPad2d -> <syft.ast.klass.Class object at 0x12fd088e0>\n",
       "\t\t.ReplicationPad3d -> <syft.ast.klass.Class object at 0x12fd08e20>\n",
       "\t\t.SELU -> <syft.ast.klass.Class object at 0x12fd0a3a0>\n",
       "\t\t.Sigmoid -> <syft.ast.klass.Class object at 0x12fd0a880>\n",
       "\t\t.Softmax -> <syft.ast.klass.Class object at 0x12fd0ac40>\n",
       "\t\t.Softmax2d -> <syft.ast.klass.Class object at 0x12fd0f040>\n",
       "\t\t.Softmin -> <syft.ast.klass.Class object at 0x12fd0f400>\n",
       "\t\t.Softplus -> <syft.ast.klass.Class object at 0x12fd0f7c0>\n",
       "\t\t.Softshrink -> <syft.ast.klass.Class object at 0x12fd0fb80>\n",
       "\t\t.Softsign -> <syft.ast.klass.Class object at 0x12fd0ff40>\n",
       "\t\t.SyncBatchNorm -> <syft.ast.klass.Class object at 0x12fd14340>\n",
       "\t\t.Tanh -> <syft.ast.klass.Class object at 0x12fd14820>\n",
       "\t\t.Tanhshrink -> <syft.ast.klass.Class object at 0x12fd14b80>\n",
       "\t\t.Threshold -> <syft.ast.klass.Class object at 0x12fd14f40>\n",
       "\t\t.Transformer -> <syft.ast.klass.Class object at 0x12fd18340>\n",
       "\t\t.TransformerDecoder -> <syft.ast.klass.Class object at 0x12fd18700>\n",
       "\t\t.TransformerDecoderLayer -> <syft.ast.klass.Class object at 0x12fd18d60>\n",
       "\t\t.TransformerEncoder -> <syft.ast.klass.Class object at 0x12fd1b3a0>\n",
       "\t\t.TransformerEncoderLayer -> <syft.ast.klass.Class object at 0x12fd1ba00>\n",
       "\t\t.Unfold -> <syft.ast.klass.Class object at 0x12fd1f040>\n",
       "\t\t.Upsample -> <syft.ast.klass.Class object at 0x12fd1f3a0>\n",
       "\t\t.UpsamplingBilinear2d -> <syft.ast.klass.Class object at 0x12fd1f760>\n",
       "\t\t.UpsamplingNearest2d -> <syft.ast.klass.Class object at 0x12fd1fdc0>\n",
       "\t\t.ZeroPad2d -> <syft.ast.klass.Class object at 0x12fd21460>\n",
       "\n",
       "\t.return_types -> Module:\n",
       "\t\t.cummax -> <syft.ast.klass.Class object at 0x12fc1e280>\n",
       "\t\t.cummin -> <syft.ast.klass.Class object at 0x12fc1e2e0>\n",
       "\t\t.eig -> <syft.ast.klass.Class object at 0x12fc1e340>\n",
       "\t\t.kthvalue -> <syft.ast.klass.Class object at 0x12fc1e3a0>\n",
       "\t\t.lstsq -> <syft.ast.klass.Class object at 0x12fc1e400>\n",
       "\t\t.slogdet -> <syft.ast.klass.Class object at 0x12fc1e460>\n",
       "\t\t.qr -> <syft.ast.klass.Class object at 0x12fc1e4c0>\n",
       "\t\t.mode -> <syft.ast.klass.Class object at 0x12fc1e520>\n",
       "\t\t.solve -> <syft.ast.klass.Class object at 0x12fc1e580>\n",
       "\t\t.sort -> <syft.ast.klass.Class object at 0x12fc1e5e0>\n",
       "\t\t.symeig -> <syft.ast.klass.Class object at 0x12fc1e640>\n",
       "\t\t.topk -> <syft.ast.klass.Class object at 0x12fc1e6a0>\n",
       "\t\t.triangular_solve -> <syft.ast.klass.Class object at 0x12fc1e760>\n",
       "\t\t.svd -> <syft.ast.klass.Class object at 0x12fc1e7c0>\n",
       "\t\t.geqrf -> <syft.ast.klass.Class object at 0x12fc1e820>\n",
       "\t\t.median -> <syft.ast.klass.Class object at 0x12fc1e880>\n",
       "\t\t.max -> <syft.ast.klass.Class object at 0x12fc1e8e0>\n",
       "\t\t.min -> <syft.ast.klass.Class object at 0x12fc1e940>\n",
       "\n",
       "\t.Size -> <syft.ast.klass.Class object at 0x12fc75b20>\n",
       "\t.set_grad_enabled -> <syft.ast.klass.Class object at 0x12fc75dc0>\n",
       "\t.zeros -> <syft.ast.callable.Callable object at 0x12fc75e20>\n",
       "\t.randn -> <syft.ast.callable.Callable object at 0x12fc75e80>\n",
       "\t.ones_like -> <syft.ast.callable.Callable object at 0x12fc75ee0>\n",
       "\t.abs_ -> <syft.ast.callable.Callable object at 0x12fc79040>\n",
       "\t.abs -> <syft.ast.callable.Callable object at 0x12fc790a0>\n",
       "\t.acos_ -> <syft.ast.callable.Callable object at 0x12fc79100>\n",
       "\t.acos -> <syft.ast.callable.Callable object at 0x12fc79160>\n",
       "\t.add -> <syft.ast.callable.Callable object at 0x12fc791c0>\n",
       "\t.addbmm -> <syft.ast.callable.Callable object at 0x12fc79220>\n",
       "\t.addcdiv -> <syft.ast.callable.Callable object at 0x12fc79280>\n",
       "\t.addcmul -> <syft.ast.callable.Callable object at 0x12fc792e0>\n",
       "\t.addmm -> <syft.ast.callable.Callable object at 0x12fc79340>\n",
       "\t.addmv_ -> <syft.ast.callable.Callable object at 0x12fc793a0>\n",
       "\t.addmv -> <syft.ast.callable.Callable object at 0x12fc79400>\n",
       "\t.addr -> <syft.ast.callable.Callable object at 0x12fc79460>\n",
       "\t.all -> <syft.ast.callable.Callable object at 0x12fc794c0>\n",
       "\t.allclose -> <syft.ast.callable.Callable object at 0x12fc79520>\n",
       "\t.angle -> <syft.ast.callable.Callable object at 0x12fc79580>\n",
       "\t.any -> <syft.ast.callable.Callable object at 0x12fc795e0>\n",
       "\t.argmax -> <syft.ast.callable.Callable object at 0x12fc79640>\n",
       "\t.argmin -> <syft.ast.callable.Callable object at 0x12fc796a0>\n",
       "\t.argsort -> <syft.ast.callable.Callable object at 0x12fc79700>\n",
       "\t.as_strided_ -> <syft.ast.callable.Callable object at 0x12fc79760>\n",
       "\t.as_strided -> <syft.ast.callable.Callable object at 0x12fc797c0>\n",
       "\t.asin_ -> <syft.ast.callable.Callable object at 0x12fc79820>\n",
       "\t.asin -> <syft.ast.callable.Callable object at 0x12fc79880>\n",
       "\t.atan_ -> <syft.ast.callable.Callable object at 0x12fc798e0>\n",
       "\t.atan -> <syft.ast.callable.Callable object at 0x12fc79940>\n",
       "\t.atan2 -> <syft.ast.callable.Callable object at 0x12fc799a0>\n",
       "\t.baddbmm -> <syft.ast.callable.Callable object at 0x12fc79a00>\n",
       "\t.bernoulli -> <syft.ast.callable.Callable object at 0x12fc79a60>\n",
       "\t.bitwise_and -> <syft.ast.callable.Callable object at 0x12fc79ac0>\n",
       "\t.bitwise_not -> <syft.ast.callable.Callable object at 0x12fc79b20>\n",
       "\t.bitwise_or -> <syft.ast.callable.Callable object at 0x12fc79b80>\n",
       "\t.bitwise_xor -> <syft.ast.callable.Callable object at 0x12fc79be0>\n",
       "\t.bmm -> <syft.ast.callable.Callable object at 0x12fc79c40>\n",
       "\t.ceil_ -> <syft.ast.callable.Callable object at 0x12fc79ca0>\n",
       "\t.ceil -> <syft.ast.callable.Callable object at 0x12fc79d00>\n",
       "\t.cholesky_inverse -> <syft.ast.callable.Callable object at 0x12fc79d60>\n",
       "\t.cholesky_solve -> <syft.ast.callable.Callable object at 0x12fc79dc0>\n",
       "\t.cholesky -> <syft.ast.callable.Callable object at 0x12fc79e20>\n",
       "\t.chunk -> <syft.ast.callable.Callable object at 0x12fc79e80>\n",
       "\t.clamp_ -> <syft.ast.callable.Callable object at 0x12fc79ee0>\n",
       "\t.clamp_max_ -> <syft.ast.callable.Callable object at 0x12fc79f40>\n",
       "\t.clamp_max -> <syft.ast.callable.Callable object at 0x12fc79fa0>\n",
       "\t.clamp_min_ -> <syft.ast.callable.Callable object at 0x12fc7e040>\n",
       "\t.clamp_min -> <syft.ast.callable.Callable object at 0x12fc7e0a0>\n",
       "\t.clamp -> <syft.ast.callable.Callable object at 0x12fc7e100>\n",
       "\t.clone -> <syft.ast.callable.Callable object at 0x12fc7e160>\n",
       "\t.conj -> <syft.ast.callable.Callable object at 0x12fc7e1c0>\n",
       "\t.cos_ -> <syft.ast.callable.Callable object at 0x12fc7e220>\n",
       "\t.cos -> <syft.ast.callable.Callable object at 0x12fc7e280>\n",
       "\t.cosh_ -> <syft.ast.callable.Callable object at 0x12fc7e2e0>\n",
       "\t.cosh -> <syft.ast.callable.Callable object at 0x12fc7e340>\n",
       "\t.cross -> <syft.ast.callable.Callable object at 0x12fc7e3a0>\n",
       "\t.cummax -> <syft.ast.callable.Callable object at 0x12fc7e400>\n",
       "\t.cummin -> <syft.ast.callable.Callable object at 0x12fc7e460>\n",
       "\t.cumprod -> <syft.ast.callable.Callable object at 0x12fc7e4c0>\n",
       "\t.cumsum -> <syft.ast.callable.Callable object at 0x12fc7e520>\n",
       "\t.dequantize -> <syft.ast.callable.Callable object at 0x12fc7e580>\n",
       "\t.det -> <syft.ast.callable.Callable object at 0x12fc7e5e0>\n",
       "\t.detach -> <syft.ast.callable.Callable object at 0x12fc7e640>\n",
       "\t.diag_embed -> <syft.ast.callable.Callable object at 0x12fc7e6a0>\n",
       "\t.diag -> <syft.ast.callable.Callable object at 0x12fc7e700>\n",
       "\t.diagflat -> <syft.ast.callable.Callable object at 0x12fc7e760>\n",
       "\t.diagonal -> <syft.ast.callable.Callable object at 0x12fc7e7c0>\n",
       "\t.digamma -> <syft.ast.callable.Callable object at 0x12fc7e820>\n",
       "\t.dist -> <syft.ast.callable.Callable object at 0x12fc7e880>\n",
       "\t.div -> <syft.ast.callable.Callable object at 0x12fc7e8e0>\n",
       "\t.dot -> <syft.ast.callable.Callable object at 0x12fc7e940>\n",
       "\t.eig -> <syft.ast.callable.Callable object at 0x12fc7e9a0>\n",
       "\t.eq -> <syft.ast.callable.Callable object at 0x12fc7ea00>\n",
       "\t.equal -> <syft.ast.callable.Callable object at 0x12fc7ea60>\n",
       "\t.erf_ -> <syft.ast.callable.Callable object at 0x12fc7eac0>\n",
       "\t.erf -> <syft.ast.callable.Callable object at 0x12fc7eb20>\n",
       "\t.erfc_ -> <syft.ast.callable.Callable object at 0x12fc7eb80>\n",
       "\t.erfc -> <syft.ast.callable.Callable object at 0x12fc7ebe0>\n",
       "\t.erfinv -> <syft.ast.callable.Callable object at 0x12fc7ec40>\n",
       "\t.exp_ -> <syft.ast.callable.Callable object at 0x12fc7eca0>\n",
       "\t.exp -> <syft.ast.callable.Callable object at 0x12fc7ed00>\n",
       "\t.expm1_ -> <syft.ast.callable.Callable object at 0x12fc7ed60>\n",
       "\t.expm1 -> <syft.ast.callable.Callable object at 0x12fc7edc0>\n",
       "\t.fft -> Module:\n",
       "\n",
       "\t.fill_ -> <syft.ast.callable.Callable object at 0x12fc7ee80>\n",
       "\t.flatten -> <syft.ast.callable.Callable object at 0x12fc7eee0>\n",
       "\t.flip -> <syft.ast.callable.Callable object at 0x12fc7ef40>\n",
       "\t.floor_ -> <syft.ast.callable.Callable object at 0x12fc7efa0>\n",
       "\t.floor_divide -> <syft.ast.callable.Callable object at 0x12fc81040>\n",
       "\t.floor -> <syft.ast.callable.Callable object at 0x12fc810a0>\n",
       "\t.fmod -> <syft.ast.callable.Callable object at 0x12fc81100>\n",
       "\t.frac_ -> <syft.ast.callable.Callable object at 0x12fc81160>\n",
       "\t.frac -> <syft.ast.callable.Callable object at 0x12fc811c0>\n",
       "\t.gather -> <syft.ast.callable.Callable object at 0x12fc81220>\n",
       "\t.ge -> <syft.ast.callable.Callable object at 0x12fc81280>\n",
       "\t.geqrf -> <syft.ast.callable.Callable object at 0x12fc812e0>\n",
       "\t.ger -> <syft.ast.callable.Callable object at 0x12fc81340>\n",
       "\t.get_device -> <syft.ast.callable.Callable object at 0x12fc813a0>\n",
       "\t.gt -> <syft.ast.callable.Callable object at 0x12fc81400>\n",
       "\t.hardshrink -> <syft.ast.callable.Callable object at 0x12fc81460>\n",
       "\t.histc -> <syft.ast.callable.Callable object at 0x12fc814c0>\n",
       "\t.index_add -> <syft.ast.callable.Callable object at 0x12fc81520>\n",
       "\t.index_copy -> <syft.ast.callable.Callable object at 0x12fc81580>\n",
       "\t.index_fill -> <syft.ast.callable.Callable object at 0x12fc815e0>\n",
       "\t.index_put_ -> <syft.ast.callable.Callable object at 0x12fc81640>\n",
       "\t.index_put -> <syft.ast.callable.Callable object at 0x12fc816a0>\n",
       "\t.index_select -> <syft.ast.callable.Callable object at 0x12fc81700>\n",
       "\t.int_repr -> <syft.ast.callable.Callable object at 0x12fc81760>\n",
       "\t.inverse -> <syft.ast.callable.Callable object at 0x12fc817c0>\n",
       "\t.is_complex -> <syft.ast.callable.Callable object at 0x12fc81820>\n",
       "\t.is_distributed -> <syft.ast.callable.Callable object at 0x12fc81880>\n",
       "\t.is_floating_point -> <syft.ast.callable.Callable object at 0x12fc818e0>\n",
       "\t.is_nonzero -> <syft.ast.callable.Callable object at 0x12fc81940>\n",
       "\t.is_same_size -> <syft.ast.callable.Callable object at 0x12fc819a0>\n",
       "\t.is_signed -> <syft.ast.callable.Callable object at 0x12fc81a00>\n",
       "\t.isclose -> <syft.ast.callable.Callable object at 0x12fc81a60>\n",
       "\t.kthvalue -> <syft.ast.callable.Callable object at 0x12fc81ac0>\n",
       "\t.le -> <syft.ast.callable.Callable object at 0x12fc81b20>\n",
       "\t.lerp -> <syft.ast.callable.Callable object at 0x12fc81b80>\n",
       "\t.lgamma -> <syft.ast.callable.Callable object at 0x12fc81be0>\n",
       "\t.log_ -> <syft.ast.callable.Callable object at 0x12fc81c40>\n",
       "\t.log_softmax -> <syft.ast.callable.Callable object at 0x12fc81ca0>\n",
       "\t.log -> <syft.ast.callable.Callable object at 0x12fc81d00>\n",
       "\t.log10_ -> <syft.ast.callable.Callable object at 0x12fc81d60>\n",
       "\t.log10 -> <syft.ast.callable.Callable object at 0x12fc81dc0>\n",
       "\t.log1p_ -> <syft.ast.callable.Callable object at 0x12fc81e20>\n",
       "\t.log1p -> <syft.ast.callable.Callable object at 0x12fc81e80>\n",
       "\t.log2_ -> <syft.ast.callable.Callable object at 0x12fc81ee0>\n",
       "\t.log2 -> <syft.ast.callable.Callable object at 0x12fc81f40>\n",
       "\t.logdet -> <syft.ast.callable.Callable object at 0x12fc81fa0>\n",
       "\t.logical_and -> <syft.ast.callable.Callable object at 0x12fc87040>\n",
       "\t.logical_not -> <syft.ast.callable.Callable object at 0x12fc870a0>\n",
       "\t.logical_or -> <syft.ast.callable.Callable object at 0x12fc87100>\n",
       "\t.logical_xor -> <syft.ast.callable.Callable object at 0x12fc87160>\n",
       "\t.logsumexp -> <syft.ast.callable.Callable object at 0x12fc871c0>\n",
       "\t.lstsq -> <syft.ast.callable.Callable object at 0x12fc87220>\n",
       "\t.lt -> <syft.ast.callable.Callable object at 0x12fc87280>\n",
       "\t.lu_solve -> <syft.ast.callable.Callable object at 0x12fc872e0>\n",
       "\t.lu -> <syft.ast.callable.Callable object at 0x12fc87340>\n",
       "\t.masked_fill -> <syft.ast.callable.Callable object at 0x12fc873a0>\n",
       "\t.masked_scatter -> <syft.ast.callable.Callable object at 0x12fc87400>\n",
       "\t.masked_select -> <syft.ast.callable.Callable object at 0x12fc87460>\n",
       "\t.matmul -> <syft.ast.callable.Callable object at 0x12fc874c0>\n",
       "\t.matrix_power -> <syft.ast.callable.Callable object at 0x12fc87520>\n",
       "\t.mean -> <syft.ast.callable.Callable object at 0x12fc87580>\n",
       "\t.mm -> <syft.ast.callable.Callable object at 0x12fc875e0>\n",
       "\t.mode -> <syft.ast.callable.Callable object at 0x12fc87640>\n",
       "\t.mul -> <syft.ast.callable.Callable object at 0x12fc876a0>\n",
       "\t.multinomial -> <syft.ast.callable.Callable object at 0x12fc87700>\n",
       "\t.mv -> <syft.ast.callable.Callable object at 0x12fc87760>\n",
       "\t.mvlgamma -> <syft.ast.callable.Callable object at 0x12fc877c0>\n",
       "\t.narrow -> <syft.ast.callable.Callable object at 0x12fc87820>\n",
       "\t.ne -> <syft.ast.callable.Callable object at 0x12fc87880>\n",
       "\t.neg_ -> <syft.ast.callable.Callable object at 0x12fc878e0>\n",
       "\t.neg -> <syft.ast.callable.Callable object at 0x12fc87940>\n",
       "\t.nonzero -> <syft.ast.callable.Callable object at 0x12fc879a0>\n",
       "\t.norm -> <syft.ast.callable.Callable object at 0x12fc87a00>\n",
       "\t.orgqr -> <syft.ast.callable.Callable object at 0x12fc87a60>\n",
       "\t.ormqr -> <syft.ast.callable.Callable object at 0x12fc87ac0>\n",
       "\t.pinverse -> <syft.ast.callable.Callable object at 0x12fc87b20>\n",
       "\t.polygamma -> <syft.ast.callable.Callable object at 0x12fc87b80>\n",
       "\t.pow -> <syft.ast.callable.Callable object at 0x12fc87be0>\n",
       "\t.prelu -> <syft.ast.callable.Callable object at 0x12fc87c40>\n",
       "\t.q_per_channel_axis -> <syft.ast.callable.Callable object at 0x12fc87ca0>\n",
       "\t.q_per_channel_scales -> <syft.ast.callable.Callable object at 0x12fc87d00>\n",
       "\t.q_per_channel_zero_points -> <syft.ast.callable.Callable object at 0x12fc87d60>\n",
       "\t.q_scale -> <syft.ast.callable.Callable object at 0x12fc87dc0>\n",
       "\t.q_zero_point -> <syft.ast.callable.Callable object at 0x12fc87e20>\n",
       "\t.qr -> <syft.ast.callable.Callable object at 0x12fc87e80>\n",
       "\t.reciprocal_ -> <syft.ast.callable.Callable object at 0x12fc87ee0>\n",
       "\t.reciprocal -> <syft.ast.callable.Callable object at 0x12fc87f40>\n",
       "\t.relu_ -> <syft.ast.callable.Callable object at 0x12fc87fa0>\n",
       "\t.relu -> <syft.ast.callable.Callable object at 0x12fc8a040>\n",
       "\t.remainder -> <syft.ast.callable.Callable object at 0x12fc8a0a0>\n",
       "\t.renorm -> <syft.ast.callable.Callable object at 0x12fc8a100>\n",
       "\t.repeat_interleave -> <syft.ast.callable.Callable object at 0x12fc8a160>\n",
       "\t.reshape -> <syft.ast.callable.Callable object at 0x12fc8a1c0>\n",
       "\t.resize_as_ -> <syft.ast.callable.Callable object at 0x12fc8a220>\n",
       "\t.roll -> <syft.ast.callable.Callable object at 0x12fc8a280>\n",
       "\t.rot90 -> <syft.ast.callable.Callable object at 0x12fc8a2e0>\n",
       "\t.round_ -> <syft.ast.callable.Callable object at 0x12fc8a340>\n",
       "\t.round -> <syft.ast.callable.Callable object at 0x12fc8a3a0>\n",
       "\t.rsqrt_ -> <syft.ast.callable.Callable object at 0x12fc8a400>\n",
       "\t.rsqrt -> <syft.ast.callable.Callable object at 0x12fc8a460>\n",
       "\t.scatter_add -> <syft.ast.callable.Callable object at 0x12fc8a4c0>\n",
       "\t.scatter -> <syft.ast.callable.Callable object at 0x12fc8a520>\n",
       "\t.select -> <syft.ast.callable.Callable object at 0x12fc8a580>\n",
       "\t.sigmoid_ -> <syft.ast.callable.Callable object at 0x12fc8a5e0>\n",
       "\t.sigmoid -> <syft.ast.callable.Callable object at 0x12fc8a640>\n",
       "\t.sign -> <syft.ast.callable.Callable object at 0x12fc8a6a0>\n",
       "\t.sin_ -> <syft.ast.callable.Callable object at 0x12fc8a700>\n",
       "\t.sin -> <syft.ast.callable.Callable object at 0x12fc8a760>\n",
       "\t.sinh_ -> <syft.ast.callable.Callable object at 0x12fc8a7c0>\n",
       "\t.sinh -> <syft.ast.callable.Callable object at 0x12fc8a820>\n",
       "\t.slogdet -> <syft.ast.callable.Callable object at 0x12fc8a880>\n",
       "\t.softmax -> <syft.ast.callable.Callable object at 0x12fc8a8e0>\n",
       "\t.solve -> <syft.ast.callable.Callable object at 0x12fc8a940>\n",
       "\t.sort -> <syft.ast.callable.Callable object at 0x12fc8a9a0>\n",
       "\t.split_with_sizes -> <syft.ast.callable.Callable object at 0x12fc8aa00>\n",
       "\t.split -> <syft.ast.callable.Callable object at 0x12fc8aa60>\n",
       "\t.sqrt_ -> <syft.ast.callable.Callable object at 0x12fc8aac0>\n",
       "\t.sqrt -> <syft.ast.callable.Callable object at 0x12fc8ab20>\n",
       "\t.square_ -> <syft.ast.callable.Callable object at 0x12fc8ab80>\n",
       "\t.square -> <syft.ast.callable.Callable object at 0x12fc8abe0>\n",
       "\t.squeeze -> <syft.ast.callable.Callable object at 0x12fc8ac40>\n",
       "\t.std -> <syft.ast.callable.Callable object at 0x12fc8aca0>\n",
       "\t.stft -> <syft.ast.callable.Callable object at 0x12fc8ad00>\n",
       "\t.sub -> <syft.ast.callable.Callable object at 0x12fc8ad60>\n",
       "\t.sum -> <syft.ast.callable.Callable object at 0x12fc8adc0>\n",
       "\t.svd -> <syft.ast.callable.Callable object at 0x12fc8ae20>\n",
       "\t.symeig -> <syft.ast.callable.Callable object at 0x12fc8ae80>\n",
       "\t.t -> <syft.ast.callable.Callable object at 0x12fc8aee0>\n",
       "\t.take -> <syft.ast.callable.Callable object at 0x12fc8af40>\n",
       "\t.tan_ -> <syft.ast.callable.Callable object at 0x12fc8afa0>\n",
       "\t.tan -> <syft.ast.callable.Callable object at 0x12fc8f040>\n",
       "\t.tanh_ -> <syft.ast.callable.Callable object at 0x12fc8f0a0>\n",
       "\t.tanh -> <syft.ast.callable.Callable object at 0x12fc8f100>\n",
       "\t.topk -> <syft.ast.callable.Callable object at 0x12fc8f160>\n",
       "\t.trace -> <syft.ast.callable.Callable object at 0x12fc8f1c0>\n",
       "\t.transpose -> <syft.ast.callable.Callable object at 0x12fc8f220>\n",
       "\t.triangular_solve -> <syft.ast.callable.Callable object at 0x12fc8f280>\n",
       "\t.tril -> <syft.ast.callable.Callable object at 0x12fc8f2e0>\n",
       "\t.triu -> <syft.ast.callable.Callable object at 0x12fc8f340>\n",
       "\t.true_divide -> <syft.ast.callable.Callable object at 0x12fc8f3a0>\n",
       "\t.trunc_ -> <syft.ast.callable.Callable object at 0x12fc8f400>\n",
       "\t.trunc -> <syft.ast.callable.Callable object at 0x12fc8f460>\n",
       "\t.unique_consecutive -> <syft.ast.callable.Callable object at 0x12fc8f4c0>\n",
       "\t.unique -> <syft.ast.callable.Callable object at 0x12fc8f520>\n",
       "\t.unsqueeze -> <syft.ast.callable.Callable object at 0x12fc8f580>\n",
       "\t.var -> <syft.ast.callable.Callable object at 0x12fc8f5e0>\n",
       "\t.unsafe_chunk -> <syft.ast.callable.Callable object at 0x12fc8f640>\n",
       "\t.absolute -> <syft.ast.callable.Callable object at 0x12fc8f6a0>\n",
       "\t.acosh_ -> <syft.ast.callable.Callable object at 0x12fc8f700>\n",
       "\t.acosh -> <syft.ast.callable.Callable object at 0x12fc8f760>\n",
       "\t.asinh_ -> <syft.ast.callable.Callable object at 0x12fc8f7c0>\n",
       "\t.asinh -> <syft.ast.callable.Callable object at 0x12fc8f820>\n",
       "\t.atanh_ -> <syft.ast.callable.Callable object at 0x12fc8f880>\n",
       "\t.atanh -> <syft.ast.callable.Callable object at 0x12fc8f8e0>\n",
       "\t.deg2rad_ -> <syft.ast.callable.Callable object at 0x12fc8f940>\n",
       "\t.deg2rad -> <syft.ast.callable.Callable object at 0x12fc8f9a0>\n",
       "\t.fliplr -> <syft.ast.callable.Callable object at 0x12fc8fa00>\n",
       "\t.flipud -> <syft.ast.callable.Callable object at 0x12fc8fa60>\n",
       "\t.isfinite -> <syft.ast.callable.Callable object at 0x12fc8fac0>\n",
       "\t.isinf -> <syft.ast.callable.Callable object at 0x12fc8fb20>\n",
       "\t.isnan -> <syft.ast.callable.Callable object at 0x12fc8fb80>\n",
       "\t.logaddexp -> <syft.ast.callable.Callable object at 0x12fc8fbe0>\n",
       "\t.logaddexp2 -> <syft.ast.callable.Callable object at 0x12fc8fc40>\n",
       "\t.logcumsumexp -> <syft.ast.callable.Callable object at 0x12fc8fca0>\n",
       "\t.rad2deg_ -> <syft.ast.callable.Callable object at 0x12fc8fd00>\n",
       "\t.rad2deg -> <syft.ast.callable.Callable object at 0x12fc8fd60>\n",
       "\t.istft -> <syft.ast.callable.Callable object at 0x12fc8fdc0>\n",
       "\t.amax -> <syft.ast.callable.Callable object at 0x12fc8fe20>\n",
       "\t.amin -> <syft.ast.callable.Callable object at 0x12fc8fe80>\n",
       "\t.arccos -> <syft.ast.callable.Callable object at 0x12fc8fee0>\n",
       "\t.arccos_ -> <syft.ast.callable.Callable object at 0x12fc8ff40>\n",
       "\t.arccosh -> <syft.ast.callable.Callable object at 0x12fc8ffa0>\n",
       "\t.arccosh_ -> <syft.ast.callable.Callable object at 0x12fc93040>\n",
       "\t.arcsin -> <syft.ast.callable.Callable object at 0x12fc930a0>\n",
       "\t.arcsin_ -> <syft.ast.callable.Callable object at 0x12fc93100>\n",
       "\t.arcsinh -> <syft.ast.callable.Callable object at 0x12fc93160>\n",
       "\t.arcsinh_ -> <syft.ast.callable.Callable object at 0x12fc931c0>\n",
       "\t.arctan -> <syft.ast.callable.Callable object at 0x12fc93220>\n",
       "\t.arctan_ -> <syft.ast.callable.Callable object at 0x12fc93280>\n",
       "\t.arctanh -> <syft.ast.callable.Callable object at 0x12fc932e0>\n",
       "\t.arctanh_ -> <syft.ast.callable.Callable object at 0x12fc93340>\n",
       "\t.clip -> <syft.ast.callable.Callable object at 0x12fc933a0>\n",
       "\t.clip_ -> <syft.ast.callable.Callable object at 0x12fc93400>\n",
       "\t.count_nonzero -> <syft.ast.callable.Callable object at 0x12fc93460>\n",
       "\t.divide -> <syft.ast.callable.Callable object at 0x12fc934c0>\n",
       "\t.exp2 -> <syft.ast.callable.Callable object at 0x12fc93580>\n",
       "\t.exp2_ -> <syft.ast.callable.Callable object at 0x12fc935e0>\n",
       "\t.fix -> <syft.ast.callable.Callable object at 0x12fc93640>\n",
       "\t.fix_ -> <syft.ast.callable.Callable object at 0x12fc936a0>\n",
       "\t.gcd -> <syft.ast.callable.Callable object at 0x12fc93700>\n",
       "\t.gcd_ -> <syft.ast.callable.Callable object at 0x12fc93760>\n",
       "\t.greater -> <syft.ast.callable.Callable object at 0x12fc937c0>\n",
       "\t.greater_equal -> <syft.ast.callable.Callable object at 0x12fc93820>\n",
       "\t.heaviside -> <syft.ast.callable.Callable object at 0x12fc93880>\n",
       "\t.hypot -> <syft.ast.callable.Callable object at 0x12fc938e0>\n",
       "\t.i0 -> <syft.ast.callable.Callable object at 0x12fc93940>\n",
       "\t.i0_ -> <syft.ast.callable.Callable object at 0x12fc939a0>\n",
       "\t.isneginf -> <syft.ast.callable.Callable object at 0x12fc93a00>\n",
       "\t.isposinf -> <syft.ast.callable.Callable object at 0x12fc93a60>\n",
       "\t.isreal -> <syft.ast.callable.Callable object at 0x12fc93ac0>\n",
       "\t.lcm -> <syft.ast.callable.Callable object at 0x12fc93b20>\n",
       "\t.lcm_ -> <syft.ast.callable.Callable object at 0x12fc93b80>\n",
       "\t.less -> <syft.ast.callable.Callable object at 0x12fc93be0>\n",
       "\t.less_equal -> <syft.ast.callable.Callable object at 0x12fc93c40>\n",
       "\t.logit -> <syft.ast.callable.Callable object at 0x12fc93ca0>\n",
       "\t.logit_ -> <syft.ast.callable.Callable object at 0x12fc93d00>\n",
       "\t.maximum -> <syft.ast.callable.Callable object at 0x12fc93d60>\n",
       "\t.minimum -> <syft.ast.callable.Callable object at 0x12fc93dc0>\n",
       "\t.matrix_exp -> <syft.ast.callable.Callable object at 0x12fc93e20>\n",
       "\t.multiply -> <syft.ast.callable.Callable object at 0x12fc93e80>\n",
       "\t.nanquantile -> <syft.ast.callable.Callable object at 0x12fc93ee0>\n",
       "\t.nansum -> <syft.ast.callable.Callable object at 0x12fc93f40>\n",
       "\t.negative -> <syft.ast.callable.Callable object at 0x12fc93fa0>\n",
       "\t.negative_ -> <syft.ast.callable.Callable object at 0x12fc98040>\n",
       "\t.nextafter -> <syft.ast.callable.Callable object at 0x12fc980a0>\n",
       "\t.outer -> <syft.ast.callable.Callable object at 0x12fc98100>\n",
       "\t.quantile -> <syft.ast.callable.Callable object at 0x12fc98160>\n",
       "\t.sgn -> <syft.ast.callable.Callable object at 0x12fc981c0>\n",
       "\t.signbit -> <syft.ast.callable.Callable object at 0x12fc98220>\n",
       "\t.subtract -> <syft.ast.callable.Callable object at 0x12fc98280>\n",
       "\t.unsafe_split -> <syft.ast.callable.Callable object at 0x12fc982e0>\n",
       "\t.vdot -> <syft.ast.callable.Callable object at 0x12fc98340>\n",
       "\t.movedim -> <syft.ast.callable.Callable object at 0x12fc983a0>\n",
       "\t.unsafe_split_with_sizes -> <syft.ast.callable.Callable object at 0x12fc98400>\n",
       "\t.cuda -> Module:\n",
       "\t\t.is_available -> <syft.ast.callable.Callable object at 0x12fc984c0>\n",
       "\n",
       "\t.device -> <syft.ast.klass.Class object at 0x12fc98520>\n",
       "\t.random -> Module:\n",
       "\t\t.initial_seed -> <syft.ast.callable.Callable object at 0x12fc986a0>\n",
       "\n",
       "\t.zeros_like -> <syft.ast.callable.Callable object at 0x12fc98700>\n",
       "\t.manual_seed -> <syft.ast.callable.Callable object at 0x12fc98760>\n",
       "\t.Generator -> <syft.ast.klass.Class object at 0x12fc987c0>\n",
       "\t.utils -> Module:\n",
       "\t\t.data -> Module:\n",
       "\t\t\t.DataLoader -> <syft.ast.klass.Class object at 0x12fc9d760>\n",
       "\t\t\t.dataloader -> Module:\n",
       "\t\t\t\t._SingleProcessDataLoaderIter -> <syft.ast.klass.Class object at 0x12fc9d9a0>\n",
       "\n",
       "\n",
       "\n",
       "\t.optim -> Module:\n",
       "\t\t.ASGD -> <syft.ast.klass.Class object at 0x12fc9dd00>\n",
       "\t\t.Adadelta -> <syft.ast.klass.Class object at 0x12fc9de80>\n",
       "\t\t.Adagrad -> <syft.ast.klass.Class object at 0x12fca3040>\n",
       "\t\t.Adam -> <syft.ast.klass.Class object at 0x12fca31c0>\n",
       "\t\t.AdamW -> <syft.ast.klass.Class object at 0x12fca3340>\n",
       "\t\t.Adamax -> <syft.ast.klass.Class object at 0x12fca34c0>\n",
       "\t\t.LBFGS -> <syft.ast.klass.Class object at 0x12fca3640>\n",
       "\t\t.Optimizer -> <syft.ast.klass.Class object at 0x12fca37c0>\n",
       "\t\t.RMSprop -> <syft.ast.klass.Class object at 0x12fca39a0>\n",
       "\t\t.Rprop -> <syft.ast.klass.Class object at 0x12fca3b20>\n",
       "\t\t.SGD -> <syft.ast.klass.Class object at 0x12fca3ca0>\n",
       "\t\t.SparseAdam -> <syft.ast.klass.Class object at 0x12fca3e20>\n",
       "\t\t.lr_scheduler -> Module:\n",
       "\t\t\t.StepLR -> <syft.ast.klass.Class object at 0x12fca7100>\n",
       "\n",
       "\n",
       "\t.no_grad -> <syft.ast.klass.Class object at 0x12fca72e0>\n",
       "\t.autograd -> Module:\n",
       "\t\t.grad_mode -> Module:\n",
       "\t\t\t.no_grad -> <syft.ast.klass.Class object at 0x12fca7460>\n",
       "\n",
       "\n",
       "\t.kron -> <syft.ast.callable.Callable object at 0x12fd21be0>\n",
       "\t.msort -> <syft.ast.callable.Callable object at 0x12fd21d00>\n",
       "\t.row_stack -> <syft.ast.callable.Callable object at 0x12fd21d60>\n",
       "\t.moveaxis -> <syft.ast.callable.Callable object at 0x12fd260a0>\n",
       "\t.tensor_split -> <syft.ast.callable.Callable object at 0x12fd261c0>\n",
       "\t.tile -> <syft.ast.callable.Callable object at 0x12fd262e0>\n",
       "\t.fmin -> <syft.ast.callable.Callable object at 0x12fd264c0>\n",
       "\t.ldexp -> <syft.ast.callable.Callable object at 0x12fd26520>\n",
       "\t.igamma -> <syft.ast.callable.Callable object at 0x12fd26640>\n",
       "\t.float_power -> <syft.ast.callable.Callable object at 0x12fd26b20>\n",
       "\t.xlogy -> <syft.ast.callable.Callable object at 0x12fd26c40>\n",
       "\t.copysign -> <syft.ast.callable.Callable object at 0x12fd26ca0>\n",
       "\t.nanmedian -> <syft.ast.callable.Callable object at 0x12fd26d00>\n",
       "\t.igammac -> <syft.ast.callable.Callable object at 0x12fd26d60>\n",
       "\t.diff -> <syft.ast.callable.Callable object at 0x12fd26e80>\n",
       "\t.sinc -> <syft.ast.callable.Callable object at 0x12fd26fa0>\n",
       "\t.column_stack -> <syft.ast.callable.Callable object at 0x12fd2b1c0>\n",
       "\t.pixel_unshuffle -> <syft.ast.callable.Callable object at 0x12fd2b2e0>\n",
       "\t.swapaxes -> <syft.ast.callable.Callable object at 0x12fd2b580>\n",
       "\t.nan_to_num -> <syft.ast.callable.Callable object at 0x12fd2b5e0>\n",
       "\t.inner -> <syft.ast.callable.Callable object at 0x12fd2b640>\n",
       "\t.fmax -> <syft.ast.callable.Callable object at 0x12fd2b6a0>\n",
       "\t.ravel -> <syft.ast.callable.Callable object at 0x12fd2ba00>\n",
       "\t.broadcast_to -> <syft.ast.callable.Callable object at 0x12fd2ba60>\n",
       "\t.swapdims -> <syft.ast.callable.Callable object at 0x12fd2bc40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duet.torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a model just like the one in the MNIST example. We do this in almost the exact same way as in PyTorch. The main difference is we inherit from sy.Module instead of nn.Module and we need to pass in a variable called torch_ref which we will use internally for any calls that would normally be to torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.conv1 = self.torch_ref.nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = self.torch_ref.nn.Conv2d(32, 64, 3, 1) \n",
    "        self.dropout1 = self.torch_ref.nn.Dropout2d(0.25)\n",
    "        self.dropout2 = self.torch_ref.nn.Dropout2d(0.5)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(9216, 128)\n",
    "        self.fc2 = self.torch_ref.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        x = self.torch_ref.nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.torch_ref.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.torch_ref.nn.functional.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import torch and torchvision just as we normally would\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create the model and pass in our local copy of torch\n",
    "local_model = SyNet(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can get our MNIST Test Set ready using our local copy of torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need some transforms for the MNIST data set\n",
    "local_transform_1 = torchvision.transforms.ToTensor()  # this converts PIL images to Tensors\n",
    "local_transform_2 = torchvision.transforms.Normalize(0.1307, 0.3081)  # this normalizes the dataset\n",
    "\n",
    "# compose our transforms\n",
    "local_transforms = torchvision.transforms.Compose([local_transform_1, local_transform_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a few settings which are from the original MNIST example command-line args\n",
    "args = {\n",
    "    \"batch_size\": 64,\n",
    "    \"test_batch_size\": 1000,\n",
    "    \"epochs\": 14,\n",
    "    \"lr\": 1.0,\n",
    "    \"gamma\": 0.7,\n",
    "    \"no_cuda\": False,\n",
    "    \"dry_run\": False,\n",
    "    \"seed\": 42, # the meaning of life\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "from syft.util import get_root_data_path\n",
    "# we will configure the test set here locally since we want to know if our Data Owner's\n",
    "# private training dataset will help us reach new SOTA results for our benchmark test set\n",
    "test_kwargs = {\n",
    "    \"batch_size\": args[\"test_batch_size\"],\n",
    "}\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(str(get_root_data_path()), train=False, download=True, transform=local_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,**test_kwargs)\n",
    "test_data_length = len(test_loader.dataset)\n",
    "print(test_data_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to send the model to our partner‚Äôs Duet Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can load normal torch model weights before sending your model.\n",
    "Try training the model and saving it at the end of the notebook and then coming back and\n",
    "reloading the weights here, or you can train the same model once using the original script\n",
    "in `original` dir and load it here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model.load(\"./duet_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = local_model.send(duet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create an alias for our partner‚Äôs torch called `remote_torch` so we can refer to the local torch as `torch` and any operation we want to do remotely as `remote_torch`. Remember, the return values from `remote_torch` are `Pointers`, not the real objects. They mostly act the same when using them with other `Pointers` but you can't mix them with local torch objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_torch = duet.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# lets ask to see if our Data Owner has CUDA\n",
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch.cuda.is_available()\n",
    "has_cuda = bool(has_cuda_ptr.get(\n",
    "    request_block=True,\n",
    "    reason=\"To run test and inference locally\",\n",
    "    timeout_secs=5,  # change to something slower\n",
    "))\n",
    "print(has_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Owner device is cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = not args[\"no_cuda\"] and has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "device = remote_torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Data Owner device is {device.type.get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    model.cuda(device)\n",
    "else:\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get our params, setup an optimizer and a scheduler just the same as the PyTorch MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = remote_torch.optim.Adadelta(params, lr=args[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = remote_torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args[\"gamma\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a training loop so we can improve our remote model. Since we want to train on remote data we should first check if the model is remote since we will be using remote_torch in this function. To check if a model is local or remote simply use the `.is_local` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, torch_ref, train_loader, optimizer, epoch, args, train_data_length):\n",
    "    # + 0.5 lets us math.ceil without the import\n",
    "    train_batches = round((train_data_length / args[\"batch_size\"]) + 0.5)\n",
    "    print(f\"> Running train in {train_batches} batches\")\n",
    "    if model.is_local:\n",
    "        print(\"Training requires remote model\")\n",
    "        return\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data_ptr, target_ptr = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_ptr)\n",
    "        loss = torch_ref.nn.functional.nll_loss(output, target_ptr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_item = loss.item()\n",
    "        train_loss = duet.python.Float(0)  # create a remote Float we can use for summation\n",
    "        train_loss += loss_item\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            local_loss = None\n",
    "            local_loss = loss_item.get(\n",
    "                reason=\"To evaluate training progress\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5\n",
    "            )\n",
    "            if local_loss is not None:\n",
    "                print(\"Train Epoch: {} {} {:.4}\".format(epoch, batch_idx, local_loss))\n",
    "            else:\n",
    "                print(\"Train Epoch: {} {} ?\".format(epoch, batch_idx))\n",
    "        if batch_idx >= train_batches - 1:\n",
    "            print(\"batch_idx >= train_batches, breaking\")\n",
    "            break\n",
    "        if args[\"dry_run\"]:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a simple test loop very similar to the original PyTorch MNIST example.\n",
    "This function should expect a remote model from our outer epoch loop, so internally we can call `get` to download the weights to do an evaluation on our machine with our local test set. Remember, if we have trained on private data, our model will require permission to download, so we should use request_block=True and make sure the Data Owner approves our requests. For the rest of this function, we will use local `torch` as we normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_local(model, torch_ref, test_loader, test_data_length):\n",
    "    # download remote model\n",
    "    if not model.is_local:\n",
    "        local_model = model.get(\n",
    "            request_block=True,\n",
    "            reason=\"test evaluation\",\n",
    "            timeout_secs=5\n",
    "        )\n",
    "    else:\n",
    "        local_model = model\n",
    "    # + 0.5 lets us math.ceil without the import\n",
    "    test_batches = round((test_data_length / args[\"test_batch_size\"]) + 0.5)\n",
    "    print(f\"> Running test_local in {test_batches} batches\")\n",
    "    local_model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    with torch_ref.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = local_model(data)\n",
    "            iter_loss = torch_ref.nn.functional.nll_loss(output, target, reduction=\"sum\").item()\n",
    "            test_loss = test_loss + iter_loss\n",
    "            pred = output.argmax(dim=1)\n",
    "            total = pred.eq(target).sum().item()\n",
    "            correct += total\n",
    "            if args[\"dry_run\"]:\n",
    "                break\n",
    "                \n",
    "            if batch_idx >= test_batches - 1:\n",
    "                print(\"batch_idx >= test_batches, breaking\")\n",
    "                break\n",
    "\n",
    "    accuracy = correct / test_data_length\n",
    "    print(f\"Test Set Accuracy: {100 * accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally just for demonstration purposes, we will get the built-in MNIST dataset but on the Data Owners side from `remote_torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need some transforms for the MNIST data set\n",
    "remote_torchvision = duet.torchvision\n",
    "\n",
    "transform_1 = remote_torchvision.transforms.ToTensor()  # this converts PIL images to Tensors\n",
    "transform_2 = remote_torchvision.transforms.Normalize(0.1307, 0.3081)  # this normalizes the dataset\n",
    "\n",
    "remote_list = duet.python.List()  # create a remote list to add the transforms to\n",
    "remote_list.append(transform_1)\n",
    "remote_list.append(transform_2)\n",
    "\n",
    "# compose our transforms\n",
    "transforms = remote_torchvision.transforms.Compose(remote_list)\n",
    "\n",
    "# The DO has kindly let us initialise a DataLoader for their training set\n",
    "train_kwargs = {\n",
    "    \"batch_size\": args[\"batch_size\"],\n",
    "}\n",
    "train_data_ptr = remote_torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=transforms)\n",
    "train_loader_ptr = remote_torch.utils.data.DataLoader(train_data_ptr,**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset size is: 60000\n"
     ]
    }
   ],
   "source": [
    "# normally we would not necessarily know the length of a remote dataset so lets ask for it\n",
    "# so we can pass that to our training loop and know when to stop\n",
    "def get_train_length(train_data_ptr):\n",
    "    train_data_length = len(train_data_ptr)\n",
    "    return train_data_length\n",
    "\n",
    "try:\n",
    "    if train_data_length is None:\n",
    "        train_data_length = get_train_length(train_data_ptr)\n",
    "except NameError:\n",
    "        train_data_length = get_train_length(train_data_ptr)\n",
    "\n",
    "print(f\"Training Dataset size is: {train_data_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch: 1\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 1 0 2.242\n",
      "Train Epoch: 1 10 1.388\n",
      "Train Epoch: 1 20 1.103\n",
      "Train Epoch: 1 30 0.4855\n",
      "Train Epoch: 1 40 0.3761\n",
      "Train Epoch: 1 50 0.3294\n",
      "Train Epoch: 1 60 0.3201\n",
      "Train Epoch: 1 70 0.3085\n",
      "Train Epoch: 1 80 0.4986\n",
      "Train Epoch: 1 90 0.2154\n",
      "Train Epoch: 1 100 0.2254\n",
      "Train Epoch: 1 110 0.2466\n",
      "Train Epoch: 1 120 0.2317\n",
      "Train Epoch: 1 130 0.2181\n",
      "Train Epoch: 1 140 0.2656\n",
      "Train Epoch: 1 150 0.1238\n",
      "Train Epoch: 1 160 0.3811\n",
      "Train Epoch: 1 170 0.1428\n",
      "Train Epoch: 1 180 0.4116\n",
      "Train Epoch: 1 190 0.1893\n",
      "Train Epoch: 1 200 0.1132\n",
      "Train Epoch: 1 210 0.1065\n",
      "Train Epoch: 1 220 0.08341\n",
      "Train Epoch: 1 230 0.3663\n",
      "Train Epoch: 1 240 0.1221\n",
      "Train Epoch: 1 250 0.256\n",
      "Train Epoch: 1 260 0.2167\n",
      "Train Epoch: 1 270 0.05931\n",
      "Train Epoch: 1 280 0.2806\n",
      "Train Epoch: 1 290 0.1996\n",
      "Train Epoch: 1 300 0.2519\n",
      "Train Epoch: 1 310 0.2816\n",
      "Train Epoch: 1 320 0.06111\n",
      "Train Epoch: 1 330 0.1067\n",
      "Train Epoch: 1 340 0.05226\n",
      "Train Epoch: 1 350 0.07378\n",
      "Train Epoch: 1 360 0.2128\n",
      "Train Epoch: 1 370 0.3184\n",
      "Train Epoch: 1 380 0.01028\n",
      "Train Epoch: 1 390 0.1243\n",
      "Train Epoch: 1 400 0.06089\n",
      "Train Epoch: 1 410 0.1098\n",
      "Train Epoch: 1 420 0.1858\n",
      "Train Epoch: 1 430 0.1867\n",
      "Train Epoch: 1 440 0.2325\n",
      "Train Epoch: 1 450 0.1282\n",
      "Train Epoch: 1 460 0.1079\n",
      "Train Epoch: 1 470 0.08072\n",
      "Train Epoch: 1 480 0.156\n",
      "Train Epoch: 1 490 0.2279\n",
      "Train Epoch: 1 500 0.1912\n",
      "Train Epoch: 1 510 0.1383\n",
      "Train Epoch: 1 520 0.09436\n",
      "Train Epoch: 1 530 0.01904\n",
      "Train Epoch: 1 540 0.03489\n",
      "Train Epoch: 1 550 0.2757\n",
      "Train Epoch: 1 560 0.159\n",
      "Train Epoch: 1 570 0.157\n",
      "Train Epoch: 1 580 0.1908\n",
      "Train Epoch: 1 590 0.1641\n",
      "Train Epoch: 1 600 0.1069\n",
      "Train Epoch: 1 610 0.03919\n",
      "Train Epoch: 1 620 0.0701\n",
      "Train Epoch: 1 630 0.04496\n",
      "Train Epoch: 1 640 0.1085\n",
      "Train Epoch: 1 650 0.1104\n",
      "Train Epoch: 1 660 0.08308\n",
      "Train Epoch: 1 670 0.1505\n",
      "Train Epoch: 1 680 0.1175\n",
      "Train Epoch: 1 690 0.08372\n",
      "Train Epoch: 1 700 0.1411\n",
      "Train Epoch: 1 710 0.1577\n",
      "Train Epoch: 1 720 0.2938\n",
      "Train Epoch: 1 730 0.1702\n",
      "Train Epoch: 1 740 0.1479\n",
      "Train Epoch: 1 750 0.08167\n",
      "Train Epoch: 1 760 0.04658\n",
      "Train Epoch: 1 770 0.06598\n",
      "Train Epoch: 1 780 0.1474\n",
      "Train Epoch: 1 790 0.1255\n",
      "Train Epoch: 1 800 0.09698\n",
      "Train Epoch: 1 810 0.03609\n",
      "Train Epoch: 1 820 0.03879\n",
      "Train Epoch: 1 830 0.1155\n",
      "Train Epoch: 1 840 0.127\n",
      "Train Epoch: 1 850 0.05449\n",
      "Train Epoch: 1 860 0.04845\n",
      "Train Epoch: 1 870 0.0686\n",
      "Train Epoch: 1 880 0.05166\n",
      "Train Epoch: 1 890 0.04212\n",
      "Train Epoch: 1 900 0.2486\n",
      "Train Epoch: 1 910 0.01962\n",
      "Train Epoch: 1 920 0.01057\n",
      "Train Epoch: 1 930 0.001807\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 98.26%\n",
      "Epoch time: 198 seconds\n",
      "Epoch: 2\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 2 0 0.04233\n",
      "Train Epoch: 2 10 0.04359\n",
      "Train Epoch: 2 20 0.06879\n",
      "Train Epoch: 2 30 0.1398\n",
      "Train Epoch: 2 40 0.07992\n",
      "Train Epoch: 2 50 0.06974\n",
      "Train Epoch: 2 60 0.1198\n",
      "Train Epoch: 2 70 0.1029\n",
      "Train Epoch: 2 80 0.0751\n",
      "Train Epoch: 2 90 0.1274\n",
      "Train Epoch: 2 100 0.1165\n",
      "Train Epoch: 2 110 0.1397\n",
      "Train Epoch: 2 120 0.08599\n",
      "Train Epoch: 2 130 0.1164\n",
      "Train Epoch: 2 140 0.06826\n",
      "Train Epoch: 2 150 0.102\n",
      "Train Epoch: 2 160 0.1319\n",
      "Train Epoch: 2 170 0.05255\n",
      "Train Epoch: 2 180 0.07465\n",
      "Train Epoch: 2 190 0.0763\n",
      "Train Epoch: 2 200 0.05083\n",
      "Train Epoch: 2 210 0.02815\n",
      "Train Epoch: 2 220 0.05481\n",
      "Train Epoch: 2 230 0.2832\n",
      "Train Epoch: 2 240 0.07529\n",
      "Train Epoch: 2 250 0.0429\n",
      "Train Epoch: 2 260 0.09405\n",
      "Train Epoch: 2 270 0.007805\n",
      "Train Epoch: 2 280 0.03044\n",
      "Train Epoch: 2 290 0.06002\n",
      "Train Epoch: 2 300 0.06129\n",
      "Train Epoch: 2 310 0.2159\n",
      "Train Epoch: 2 320 0.01506\n",
      "Train Epoch: 2 330 0.05178\n",
      "Train Epoch: 2 340 0.006343\n",
      "Train Epoch: 2 350 0.01077\n",
      "Train Epoch: 2 360 0.1598\n",
      "Train Epoch: 2 370 0.0385\n",
      "Train Epoch: 2 380 0.003805\n",
      "Train Epoch: 2 390 0.05766\n",
      "Train Epoch: 2 400 0.01906\n",
      "Train Epoch: 2 410 0.01695\n",
      "Train Epoch: 2 420 0.1586\n",
      "Train Epoch: 2 430 0.1988\n",
      "Train Epoch: 2 440 0.1384\n",
      "Train Epoch: 2 450 0.07079\n",
      "Train Epoch: 2 460 0.02939\n",
      "Train Epoch: 2 470 0.0717\n",
      "Train Epoch: 2 480 0.04901\n",
      "Train Epoch: 2 490 0.05029\n",
      "Train Epoch: 2 500 0.1495\n",
      "Train Epoch: 2 510 0.03521\n",
      "Train Epoch: 2 520 0.1358\n",
      "Train Epoch: 2 530 0.005998\n",
      "Train Epoch: 2 540 0.01693\n",
      "Train Epoch: 2 550 0.2214\n",
      "Train Epoch: 2 560 0.1559\n",
      "Train Epoch: 2 570 0.0324\n",
      "Train Epoch: 2 580 0.07625\n",
      "Train Epoch: 2 590 0.04817\n",
      "Train Epoch: 2 600 0.1049\n",
      "Train Epoch: 2 610 0.01202\n",
      "Train Epoch: 2 620 0.03748\n",
      "Train Epoch: 2 630 0.07073\n",
      "Train Epoch: 2 640 0.1111\n",
      "Train Epoch: 2 650 0.02173\n",
      "Train Epoch: 2 660 0.0106\n",
      "Train Epoch: 2 670 0.05041\n",
      "Train Epoch: 2 680 0.1076\n",
      "Train Epoch: 2 690 0.03755\n",
      "Train Epoch: 2 700 0.04903\n",
      "Train Epoch: 2 710 0.09486\n",
      "Train Epoch: 2 720 0.07678\n",
      "Train Epoch: 2 730 0.08287\n",
      "Train Epoch: 2 740 0.05958\n",
      "Train Epoch: 2 750 0.06922\n",
      "Train Epoch: 2 760 0.04011\n",
      "Train Epoch: 2 770 0.02054\n",
      "Train Epoch: 2 780 0.0685\n",
      "Train Epoch: 2 790 0.06334\n",
      "Train Epoch: 2 800 0.1926\n",
      "Train Epoch: 2 810 0.03442\n",
      "Train Epoch: 2 820 0.01386\n",
      "Train Epoch: 2 830 0.04921\n",
      "Train Epoch: 2 840 0.1018\n",
      "Train Epoch: 2 850 0.03129\n",
      "Train Epoch: 2 860 0.0456\n",
      "Train Epoch: 2 870 0.0653\n",
      "Train Epoch: 2 880 0.09811\n",
      "Train Epoch: 2 890 0.07065\n",
      "Train Epoch: 2 900 0.07887\n",
      "Train Epoch: 2 910 0.02623\n",
      "Train Epoch: 2 920 0.01801\n",
      "Train Epoch: 2 930 0.000951\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 98.7%\n",
      "Epoch time: 185 seconds\n",
      "Epoch: 3\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 3 0 0.02882\n",
      "Train Epoch: 3 10 0.01726\n",
      "Train Epoch: 3 20 0.04072\n",
      "Train Epoch: 3 30 0.1045\n",
      "Train Epoch: 3 40 0.1046\n",
      "Train Epoch: 3 50 0.1342\n",
      "Train Epoch: 3 60 0.02658\n",
      "Train Epoch: 3 70 0.007299\n",
      "Train Epoch: 3 80 0.1132\n",
      "Train Epoch: 3 90 0.04908\n",
      "Train Epoch: 3 100 0.1529\n",
      "Train Epoch: 3 110 0.177\n",
      "Train Epoch: 3 120 0.04698\n",
      "Train Epoch: 3 130 0.02209\n",
      "Train Epoch: 3 140 0.1798\n",
      "Train Epoch: 3 150 0.05089\n",
      "Train Epoch: 3 160 0.127\n",
      "Train Epoch: 3 170 0.02298\n",
      "Train Epoch: 3 180 0.04602\n",
      "Train Epoch: 3 190 0.05861\n",
      "Train Epoch: 3 200 0.1061\n",
      "Train Epoch: 3 210 0.07149\n",
      "Train Epoch: 3 220 0.01805\n",
      "Train Epoch: 3 230 0.2184\n",
      "Train Epoch: 3 240 0.01699\n",
      "Train Epoch: 3 250 0.08833\n",
      "Train Epoch: 3 260 0.07607\n",
      "Train Epoch: 3 270 0.001494\n",
      "Train Epoch: 3 280 0.02487\n",
      "Train Epoch: 3 290 0.002447\n",
      "Train Epoch: 3 300 0.1663\n",
      "Train Epoch: 3 310 0.1021\n",
      "Train Epoch: 3 320 0.01899\n",
      "Train Epoch: 3 330 0.04814\n",
      "Train Epoch: 3 340 0.02211\n",
      "Train Epoch: 3 350 0.01427\n",
      "Train Epoch: 3 360 0.1064\n",
      "Train Epoch: 3 370 0.08421\n",
      "Train Epoch: 3 380 0.01617\n",
      "Train Epoch: 3 390 0.05672\n",
      "Train Epoch: 3 400 0.01834\n",
      "Train Epoch: 3 410 0.01804\n",
      "Train Epoch: 3 420 0.1769\n",
      "Train Epoch: 3 430 0.06893\n",
      "Train Epoch: 3 440 0.005122\n",
      "Train Epoch: 3 450 0.0511\n",
      "Train Epoch: 3 460 0.04829\n",
      "Train Epoch: 3 470 0.07014\n",
      "Train Epoch: 3 480 0.02307\n",
      "Train Epoch: 3 490 0.02451\n",
      "Train Epoch: 3 500 0.03515\n",
      "Train Epoch: 3 510 0.09415\n",
      "Train Epoch: 3 520 0.122\n",
      "Train Epoch: 3 530 0.005303\n",
      "Train Epoch: 3 540 0.01852\n",
      "Train Epoch: 3 550 0.1078\n",
      "Train Epoch: 3 560 0.07489\n",
      "Train Epoch: 3 570 0.02165\n",
      "Train Epoch: 3 580 0.04097\n",
      "Train Epoch: 3 590 0.09431\n",
      "Train Epoch: 3 600 0.097\n",
      "Train Epoch: 3 610 0.003268\n",
      "Train Epoch: 3 620 0.05849\n",
      "Train Epoch: 3 630 0.03953\n",
      "Train Epoch: 3 640 0.03025\n",
      "Train Epoch: 3 650 0.03743\n",
      "Train Epoch: 3 660 0.02822\n",
      "Train Epoch: 3 670 0.01049\n",
      "Train Epoch: 3 680 0.05811\n",
      "Train Epoch: 3 690 0.02776\n",
      "Train Epoch: 3 700 0.1001\n",
      "Train Epoch: 3 710 0.09283\n",
      "Train Epoch: 3 720 0.1376\n",
      "Train Epoch: 3 730 0.1799\n",
      "Train Epoch: 3 740 0.01823\n",
      "Train Epoch: 3 750 0.02192\n",
      "Train Epoch: 3 760 0.1092\n",
      "Train Epoch: 3 770 0.01947\n",
      "Train Epoch: 3 780 0.06488\n",
      "Train Epoch: 3 790 0.04004\n",
      "Train Epoch: 3 800 0.2649\n",
      "Train Epoch: 3 810 0.05985\n",
      "Train Epoch: 3 820 0.009408\n",
      "Train Epoch: 3 830 0.03606\n",
      "Train Epoch: 3 840 0.1353\n",
      "Train Epoch: 3 850 0.06692\n",
      "Train Epoch: 3 860 0.01065\n",
      "Train Epoch: 3 870 0.0493\n",
      "Train Epoch: 3 880 0.006641\n",
      "Train Epoch: 3 890 0.0186\n",
      "Train Epoch: 3 900 0.07172\n",
      "Train Epoch: 3 910 0.05809\n",
      "Train Epoch: 3 920 0.002174\n",
      "Train Epoch: 3 930 0.003812\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 98.86%\n",
      "Epoch time: 188 seconds\n",
      "Epoch: 4\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 4 0 0.01667\n",
      "Train Epoch: 4 10 0.02183\n",
      "Train Epoch: 4 20 0.08664\n",
      "Train Epoch: 4 30 0.04215\n",
      "Train Epoch: 4 40 0.03506\n",
      "Train Epoch: 4 50 0.05981\n",
      "Train Epoch: 4 60 0.002134\n",
      "Train Epoch: 4 70 0.03043\n",
      "Train Epoch: 4 80 0.08924\n",
      "Train Epoch: 4 90 0.03756\n",
      "Train Epoch: 4 100 0.3067\n",
      "Train Epoch: 4 110 0.1414\n",
      "Train Epoch: 4 120 0.01919\n",
      "Train Epoch: 4 130 0.02193\n",
      "Train Epoch: 4 140 0.04718\n",
      "Train Epoch: 4 150 0.08055\n",
      "Train Epoch: 4 160 0.1721\n",
      "Train Epoch: 4 170 0.01408\n",
      "Train Epoch: 4 180 0.1222\n",
      "Train Epoch: 4 190 0.06818\n",
      "Train Epoch: 4 200 0.08413\n",
      "Train Epoch: 4 210 0.03434\n",
      "Train Epoch: 4 220 0.01607\n",
      "Train Epoch: 4 230 0.07407\n",
      "Train Epoch: 4 240 0.02437\n",
      "Train Epoch: 4 250 0.07071\n",
      "Train Epoch: 4 260 0.1488\n",
      "Train Epoch: 4 270 0.002952\n",
      "Train Epoch: 4 280 0.07261\n",
      "Train Epoch: 4 290 0.00847\n",
      "Train Epoch: 4 300 0.1174\n",
      "Train Epoch: 4 310 0.0646\n",
      "Train Epoch: 4 320 0.01739\n",
      "Train Epoch: 4 330 0.06287\n",
      "Train Epoch: 4 340 0.005724\n",
      "Train Epoch: 4 350 0.07261\n",
      "Train Epoch: 4 360 0.03105\n",
      "Train Epoch: 4 370 0.02293\n",
      "Train Epoch: 4 380 0.001604\n",
      "Train Epoch: 4 390 0.02802\n",
      "Train Epoch: 4 400 0.01441\n",
      "Train Epoch: 4 410 0.005918\n",
      "Train Epoch: 4 420 0.1005\n",
      "Train Epoch: 4 430 0.1242\n",
      "Train Epoch: 4 440 0.05749\n",
      "Train Epoch: 4 450 0.07537\n",
      "Train Epoch: 4 460 0.04949\n",
      "Train Epoch: 4 470 0.03591\n",
      "Train Epoch: 4 480 0.09713\n",
      "Train Epoch: 4 490 0.01151\n",
      "Train Epoch: 4 500 0.04122\n",
      "Train Epoch: 4 510 0.08816\n",
      "Train Epoch: 4 520 0.05239\n",
      "Train Epoch: 4 530 0.003917\n",
      "Train Epoch: 4 540 0.02556\n",
      "Train Epoch: 4 550 0.1248\n",
      "Train Epoch: 4 560 0.02229\n",
      "Train Epoch: 4 570 0.01364\n",
      "Train Epoch: 4 580 0.0165\n",
      "Train Epoch: 4 590 0.1011\n",
      "Train Epoch: 4 600 0.07692\n",
      "Train Epoch: 4 610 0.003451\n",
      "Train Epoch: 4 620 0.06995\n",
      "Train Epoch: 4 630 0.0729\n",
      "Train Epoch: 4 640 0.07143\n",
      "Train Epoch: 4 650 0.02218\n",
      "Train Epoch: 4 660 0.01512\n",
      "Train Epoch: 4 670 0.02043\n",
      "Train Epoch: 4 680 0.03954\n",
      "Train Epoch: 4 690 0.009338\n",
      "Train Epoch: 4 700 0.01848\n",
      "Train Epoch: 4 710 0.1224\n",
      "Train Epoch: 4 720 0.02668\n",
      "Train Epoch: 4 730 0.139\n",
      "Train Epoch: 4 740 0.03826\n",
      "Train Epoch: 4 750 0.03814\n",
      "Train Epoch: 4 760 0.04044\n",
      "Train Epoch: 4 770 0.04162\n",
      "Train Epoch: 4 780 0.07303\n",
      "Train Epoch: 4 790 0.02756\n",
      "Train Epoch: 4 800 0.0494\n",
      "Train Epoch: 4 810 0.006337\n",
      "Train Epoch: 4 820 0.008281\n",
      "Train Epoch: 4 830 0.08876\n",
      "Train Epoch: 4 840 0.05381\n",
      "Train Epoch: 4 850 0.01976\n",
      "Train Epoch: 4 860 0.009365\n",
      "Train Epoch: 4 870 0.04781\n",
      "Train Epoch: 4 880 0.06209\n",
      "Train Epoch: 4 890 0.05917\n",
      "Train Epoch: 4 900 0.02712\n",
      "Train Epoch: 4 910 0.0002772\n",
      "Train Epoch: 4 920 0.0007285\n",
      "Train Epoch: 4 930 0.003361\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.09%\n",
      "Epoch time: 180 seconds\n",
      "Epoch: 5\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 5 0 0.03012\n",
      "Train Epoch: 5 10 0.01261\n",
      "Train Epoch: 5 20 0.03249\n",
      "Train Epoch: 5 30 0.02402\n",
      "Train Epoch: 5 40 0.01186\n",
      "Train Epoch: 5 50 0.04697\n",
      "Train Epoch: 5 60 0.003626\n",
      "Train Epoch: 5 70 0.04972\n",
      "Train Epoch: 5 80 0.01659\n",
      "Train Epoch: 5 90 0.0172\n",
      "Train Epoch: 5 100 0.1502\n",
      "Train Epoch: 5 110 0.1512\n",
      "Train Epoch: 5 120 0.04156\n",
      "Train Epoch: 5 130 0.04197\n",
      "Train Epoch: 5 140 0.1678\n",
      "Train Epoch: 5 150 0.06098\n",
      "Train Epoch: 5 160 0.1296\n",
      "Train Epoch: 5 170 0.01205\n",
      "Train Epoch: 5 180 0.01181\n",
      "Train Epoch: 5 190 0.1017\n",
      "Train Epoch: 5 200 0.01019\n",
      "Train Epoch: 5 210 0.001772\n",
      "Train Epoch: 5 220 0.009817\n",
      "Train Epoch: 5 230 0.1248\n",
      "Train Epoch: 5 240 0.01222\n",
      "Train Epoch: 5 250 0.05884\n",
      "Train Epoch: 5 260 0.06034\n",
      "Train Epoch: 5 270 0.01134\n",
      "Train Epoch: 5 280 0.03015\n",
      "Train Epoch: 5 290 0.007044\n",
      "Train Epoch: 5 300 0.0468\n",
      "Train Epoch: 5 310 0.08999\n",
      "Train Epoch: 5 320 0.01209\n",
      "Train Epoch: 5 330 0.03255\n",
      "Train Epoch: 5 340 0.00161\n",
      "Train Epoch: 5 350 0.02086\n",
      "Train Epoch: 5 360 0.07355\n",
      "Train Epoch: 5 370 0.0958\n",
      "Train Epoch: 5 380 0.002428\n",
      "Train Epoch: 5 390 0.0141\n",
      "Train Epoch: 5 400 0.01789\n",
      "Train Epoch: 5 410 0.01131\n",
      "Train Epoch: 5 420 0.1405\n",
      "Train Epoch: 5 430 0.183\n",
      "Train Epoch: 5 440 0.1564\n",
      "Train Epoch: 5 450 0.01978\n",
      "Train Epoch: 5 460 0.01477\n",
      "Train Epoch: 5 470 0.01665\n",
      "Train Epoch: 5 480 0.07627\n",
      "Train Epoch: 5 490 0.03657\n",
      "Train Epoch: 5 500 0.04386\n",
      "Train Epoch: 5 510 0.01657\n",
      "Train Epoch: 5 520 0.03105\n",
      "Train Epoch: 5 530 0.003653\n",
      "Train Epoch: 5 540 0.005303\n",
      "Train Epoch: 5 550 0.1024\n",
      "Train Epoch: 5 560 0.1311\n",
      "Train Epoch: 5 570 0.009513\n",
      "Train Epoch: 5 580 0.0154\n",
      "Train Epoch: 5 590 0.04184\n",
      "Train Epoch: 5 600 0.05072\n",
      "Train Epoch: 5 610 0.001529\n",
      "Train Epoch: 5 620 0.01043\n",
      "Train Epoch: 5 630 0.01415\n",
      "Train Epoch: 5 640 0.09219\n",
      "Train Epoch: 5 650 0.009649\n",
      "Train Epoch: 5 660 0.02008\n",
      "Train Epoch: 5 670 0.03267\n",
      "Train Epoch: 5 680 0.07855\n",
      "Train Epoch: 5 690 0.009517\n",
      "Train Epoch: 5 700 0.06646\n",
      "Train Epoch: 5 710 0.08238\n",
      "Train Epoch: 5 720 0.04052\n",
      "Train Epoch: 5 730 0.07293\n",
      "Train Epoch: 5 740 0.1133\n",
      "Train Epoch: 5 750 0.03279\n",
      "Train Epoch: 5 760 0.005012\n",
      "Train Epoch: 5 770 0.002632\n",
      "Train Epoch: 5 780 0.01958\n",
      "Train Epoch: 5 790 0.04649\n",
      "Train Epoch: 5 800 0.138\n",
      "Train Epoch: 5 810 0.006428\n",
      "Train Epoch: 5 820 0.002735\n",
      "Train Epoch: 5 830 0.02445\n",
      "Train Epoch: 5 840 0.05549\n",
      "Train Epoch: 5 850 0.05552\n",
      "Train Epoch: 5 860 0.01765\n",
      "Train Epoch: 5 870 0.05029\n",
      "Train Epoch: 5 880 0.01127\n",
      "Train Epoch: 5 890 0.009976\n",
      "Train Epoch: 5 900 0.0207\n",
      "Train Epoch: 5 910 0.002224\n",
      "Train Epoch: 5 920 0.002535\n",
      "Train Epoch: 5 930 0.0001092\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.00999999999999%\n",
      "Epoch time: 173 seconds\n",
      "Epoch: 6\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 6 0 0.008623\n",
      "Train Epoch: 6 10 0.01175\n",
      "Train Epoch: 6 20 0.0203\n",
      "Train Epoch: 6 30 0.01677\n",
      "Train Epoch: 6 40 0.04222\n",
      "Train Epoch: 6 50 0.02709\n",
      "Train Epoch: 6 60 0.002104\n",
      "Train Epoch: 6 70 0.01266\n",
      "Train Epoch: 6 80 0.04418\n",
      "Train Epoch: 6 90 0.02929\n",
      "Train Epoch: 6 100 0.09751\n",
      "Train Epoch: 6 110 0.08036\n",
      "Train Epoch: 6 120 0.02246\n",
      "Train Epoch: 6 130 0.0257\n",
      "Train Epoch: 6 140 0.1141\n",
      "Train Epoch: 6 150 0.08513\n",
      "Train Epoch: 6 160 0.1506\n",
      "Train Epoch: 6 170 0.02226\n",
      "Train Epoch: 6 180 0.03604\n",
      "Train Epoch: 6 190 0.01451\n",
      "Train Epoch: 6 200 0.0202\n",
      "Train Epoch: 6 210 0.006978\n",
      "Train Epoch: 6 220 0.0274\n",
      "Train Epoch: 6 230 0.1223\n",
      "Train Epoch: 6 240 0.008011\n",
      "Train Epoch: 6 250 0.04061\n",
      "Train Epoch: 6 260 0.09773\n",
      "Train Epoch: 6 270 0.0004071\n",
      "Train Epoch: 6 280 0.02732\n",
      "Train Epoch: 6 290 0.00825\n",
      "Train Epoch: 6 300 0.09941\n",
      "Train Epoch: 6 310 0.03855\n",
      "Train Epoch: 6 320 0.001695\n",
      "Train Epoch: 6 330 0.009006\n",
      "Train Epoch: 6 340 0.008381\n",
      "Train Epoch: 6 350 0.01961\n",
      "Train Epoch: 6 360 0.0897\n",
      "Train Epoch: 6 370 0.1497\n",
      "Train Epoch: 6 380 0.0005725\n",
      "Train Epoch: 6 390 0.01821\n",
      "Train Epoch: 6 400 0.002175\n",
      "Train Epoch: 6 410 0.01593\n",
      "Train Epoch: 6 420 0.007958\n",
      "Train Epoch: 6 430 0.108\n",
      "Train Epoch: 6 440 0.02978\n",
      "Train Epoch: 6 450 0.03742\n",
      "Train Epoch: 6 460 0.00824\n",
      "Train Epoch: 6 470 0.01133\n",
      "Train Epoch: 6 480 0.06907\n",
      "Train Epoch: 6 490 0.09877\n",
      "Train Epoch: 6 500 0.01812\n",
      "Train Epoch: 6 510 0.01803\n",
      "Train Epoch: 6 520 0.0238\n",
      "Train Epoch: 6 530 0.0006383\n",
      "Train Epoch: 6 540 0.01433\n",
      "Train Epoch: 6 550 0.07549\n",
      "Train Epoch: 6 560 0.1538\n",
      "Train Epoch: 6 570 0.008751\n",
      "Train Epoch: 6 580 0.02215\n",
      "Train Epoch: 6 590 0.02865\n",
      "Train Epoch: 6 600 0.1046\n",
      "Train Epoch: 6 610 0.003561\n",
      "Train Epoch: 6 620 0.03479\n",
      "Train Epoch: 6 630 0.03082\n",
      "Train Epoch: 6 640 0.03169\n",
      "Train Epoch: 6 650 0.05769\n",
      "Train Epoch: 6 660 0.01694\n",
      "Train Epoch: 6 670 0.01434\n",
      "Train Epoch: 6 680 0.02991\n",
      "Train Epoch: 6 690 0.01108\n",
      "Train Epoch: 6 700 0.06128\n",
      "Train Epoch: 6 710 0.03328\n",
      "Train Epoch: 6 720 0.0687\n",
      "Train Epoch: 6 730 0.08616\n",
      "Train Epoch: 6 740 0.04239\n",
      "Train Epoch: 6 750 0.0535\n",
      "Train Epoch: 6 760 0.01915\n",
      "Train Epoch: 6 770 0.0181\n",
      "Train Epoch: 6 780 0.05607\n",
      "Train Epoch: 6 790 0.005151\n",
      "Train Epoch: 6 800 0.04393\n",
      "Train Epoch: 6 810 0.04201\n",
      "Train Epoch: 6 820 0.001918\n",
      "Train Epoch: 6 830 0.04275\n",
      "Train Epoch: 6 840 0.1101\n",
      "Train Epoch: 6 850 0.05317\n",
      "Train Epoch: 6 860 0.0171\n",
      "Train Epoch: 6 870 0.09171\n",
      "Train Epoch: 6 880 0.02033\n",
      "Train Epoch: 6 890 0.00651\n",
      "Train Epoch: 6 900 0.06961\n",
      "Train Epoch: 6 910 0.002548\n",
      "Train Epoch: 6 920 0.0002304\n",
      "Train Epoch: 6 930 0.0002941\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.08%\n",
      "Epoch time: 174 seconds\n",
      "Epoch: 7\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 7 0 0.005197\n",
      "Train Epoch: 7 10 0.006951\n",
      "Train Epoch: 7 20 0.007283\n",
      "Train Epoch: 7 30 0.05023\n",
      "Train Epoch: 7 40 0.01841\n",
      "Train Epoch: 7 50 0.02413\n",
      "Train Epoch: 7 60 0.002721\n",
      "Train Epoch: 7 70 0.02109\n",
      "Train Epoch: 7 80 0.09823\n",
      "Train Epoch: 7 90 0.03826\n",
      "Train Epoch: 7 100 0.1804\n",
      "Train Epoch: 7 110 0.158\n",
      "Train Epoch: 7 120 0.008846\n",
      "Train Epoch: 7 130 0.01084\n",
      "Train Epoch: 7 140 0.02612\n",
      "Train Epoch: 7 150 0.00712\n",
      "Train Epoch: 7 160 0.1512\n",
      "Train Epoch: 7 170 0.002913\n",
      "Train Epoch: 7 180 0.05654\n",
      "Train Epoch: 7 190 0.003778\n",
      "Train Epoch: 7 200 0.0258\n",
      "Train Epoch: 7 210 0.007829\n",
      "Train Epoch: 7 220 0.02217\n",
      "Train Epoch: 7 230 0.04521\n",
      "Train Epoch: 7 240 0.06307\n",
      "Train Epoch: 7 250 0.06307\n",
      "Train Epoch: 7 260 0.06092\n",
      "Train Epoch: 7 270 0.002171\n",
      "Train Epoch: 7 280 0.01065\n",
      "Train Epoch: 7 290 0.003968\n",
      "Train Epoch: 7 300 0.003962\n",
      "Train Epoch: 7 310 0.03692\n",
      "Train Epoch: 7 320 0.02816\n",
      "Train Epoch: 7 330 0.008682\n",
      "Train Epoch: 7 340 0.007958\n",
      "Train Epoch: 7 350 0.01268\n",
      "Train Epoch: 7 360 0.08858\n",
      "Train Epoch: 7 370 0.03584\n",
      "Train Epoch: 7 380 0.00231\n",
      "Train Epoch: 7 390 0.002036\n",
      "Train Epoch: 7 400 0.02309\n",
      "Train Epoch: 7 410 0.02931\n",
      "Train Epoch: 7 420 0.0808\n",
      "Train Epoch: 7 430 0.2076\n",
      "Train Epoch: 7 440 0.05391\n",
      "Train Epoch: 7 450 0.02662\n",
      "Train Epoch: 7 460 0.01972\n",
      "Train Epoch: 7 470 0.02733\n",
      "Train Epoch: 7 480 0.01273\n",
      "Train Epoch: 7 490 0.009121\n",
      "Train Epoch: 7 500 0.02585\n",
      "Train Epoch: 7 510 0.03181\n",
      "Train Epoch: 7 520 0.008884\n",
      "Train Epoch: 7 530 0.002044\n",
      "Train Epoch: 7 540 0.01633\n",
      "Train Epoch: 7 550 0.1134\n",
      "Train Epoch: 7 560 0.1053\n",
      "Train Epoch: 7 570 0.004447\n",
      "Train Epoch: 7 580 0.003937\n",
      "Train Epoch: 7 590 0.0764\n",
      "Train Epoch: 7 600 0.05569\n",
      "Train Epoch: 7 610 0.001472\n",
      "Train Epoch: 7 620 0.01554\n",
      "Train Epoch: 7 630 0.02987\n",
      "Train Epoch: 7 640 0.09915\n",
      "Train Epoch: 7 650 0.007894\n",
      "Train Epoch: 7 660 0.01816\n",
      "Train Epoch: 7 670 0.007647\n",
      "Train Epoch: 7 680 0.02288\n",
      "Train Epoch: 7 690 0.007081\n",
      "Train Epoch: 7 700 0.04166\n",
      "Train Epoch: 7 710 0.01934\n",
      "Train Epoch: 7 720 0.03449\n",
      "Train Epoch: 7 730 0.1408\n",
      "Train Epoch: 7 740 0.05981\n",
      "Train Epoch: 7 750 0.04761\n",
      "Train Epoch: 7 760 0.02557\n",
      "Train Epoch: 7 770 0.02077\n",
      "Train Epoch: 7 780 0.008282\n",
      "Train Epoch: 7 790 0.01009\n",
      "Train Epoch: 7 800 0.07513\n",
      "Train Epoch: 7 810 0.001238\n",
      "Train Epoch: 7 820 0.002\n",
      "Train Epoch: 7 830 0.03177\n",
      "Train Epoch: 7 840 0.05848\n",
      "Train Epoch: 7 850 0.0394\n",
      "Train Epoch: 7 860 0.01062\n",
      "Train Epoch: 7 870 0.006078\n",
      "Train Epoch: 7 880 0.04822\n",
      "Train Epoch: 7 890 0.006283\n",
      "Train Epoch: 7 900 0.08301\n",
      "Train Epoch: 7 910 0.003398\n",
      "Train Epoch: 7 920 0.002854\n",
      "Train Epoch: 7 930 0.001423\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.13%\n",
      "Epoch time: 176 seconds\n",
      "Epoch: 8\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 8 0 0.02079\n",
      "Train Epoch: 8 10 0.0275\n",
      "Train Epoch: 8 20 0.01915\n",
      "Train Epoch: 8 30 0.03051\n",
      "Train Epoch: 8 40 0.0187\n",
      "Train Epoch: 8 50 0.01099\n",
      "Train Epoch: 8 60 0.002852\n",
      "Train Epoch: 8 70 0.008217\n",
      "Train Epoch: 8 80 0.02701\n",
      "Train Epoch: 8 90 0.01806\n",
      "Train Epoch: 8 100 0.2196\n",
      "Train Epoch: 8 110 0.1577\n",
      "Train Epoch: 8 120 0.008477\n",
      "Train Epoch: 8 130 0.02827\n",
      "Train Epoch: 8 140 0.04457\n",
      "Train Epoch: 8 150 0.04832\n",
      "Train Epoch: 8 160 0.1235\n",
      "Train Epoch: 8 170 0.01397\n",
      "Train Epoch: 8 180 0.06069\n",
      "Train Epoch: 8 190 0.01294\n",
      "Train Epoch: 8 200 0.003556\n",
      "Train Epoch: 8 210 0.006955\n",
      "Train Epoch: 8 220 0.01032\n",
      "Train Epoch: 8 230 0.09133\n",
      "Train Epoch: 8 240 0.02488\n",
      "Train Epoch: 8 250 0.04024\n",
      "Train Epoch: 8 260 0.1166\n",
      "Train Epoch: 8 270 0.00657\n",
      "Train Epoch: 8 280 0.008853\n",
      "Train Epoch: 8 290 0.03179\n",
      "Train Epoch: 8 300 0.007542\n",
      "Train Epoch: 8 310 0.09177\n",
      "Train Epoch: 8 320 0.01304\n",
      "Train Epoch: 8 330 0.04412\n",
      "Train Epoch: 8 340 0.007433\n",
      "Train Epoch: 8 350 0.005228\n",
      "Train Epoch: 8 360 0.08667\n",
      "Train Epoch: 8 370 0.06372\n",
      "Train Epoch: 8 380 0.004092\n",
      "Train Epoch: 8 390 0.02984\n",
      "Train Epoch: 8 400 0.01287\n",
      "Train Epoch: 8 410 0.007515\n",
      "Train Epoch: 8 420 0.03328\n",
      "Train Epoch: 8 430 0.08433\n",
      "Train Epoch: 8 440 0.06136\n",
      "Train Epoch: 8 450 0.0104\n",
      "Train Epoch: 8 460 0.007837\n",
      "Train Epoch: 8 470 0.006152\n",
      "Train Epoch: 8 480 0.03841\n",
      "Train Epoch: 8 490 0.0166\n",
      "Train Epoch: 8 500 0.07127\n",
      "Train Epoch: 8 510 0.04043\n",
      "Train Epoch: 8 520 0.0167\n",
      "Train Epoch: 8 530 0.002805\n",
      "Train Epoch: 8 540 0.01228\n",
      "Train Epoch: 8 550 0.09843\n",
      "Train Epoch: 8 560 0.1553\n",
      "Train Epoch: 8 570 0.007629\n",
      "Train Epoch: 8 580 0.006763\n",
      "Train Epoch: 8 590 0.02924\n",
      "Train Epoch: 8 600 0.04431\n",
      "Train Epoch: 8 610 0.001419\n",
      "Train Epoch: 8 620 0.02238\n",
      "Train Epoch: 8 630 0.04306\n",
      "Train Epoch: 8 640 0.02847\n",
      "Train Epoch: 8 650 0.01163\n",
      "Train Epoch: 8 660 0.05955\n",
      "Train Epoch: 8 670 0.002801\n",
      "Train Epoch: 8 680 0.0493\n",
      "Train Epoch: 8 690 0.0008329\n",
      "Train Epoch: 8 700 0.02833\n",
      "Train Epoch: 8 710 0.03086\n",
      "Train Epoch: 8 720 0.03029\n",
      "Train Epoch: 8 730 0.08095\n",
      "Train Epoch: 8 740 0.04022\n",
      "Train Epoch: 8 750 0.02196\n",
      "Train Epoch: 8 760 0.02263\n",
      "Train Epoch: 8 770 0.006316\n",
      "Train Epoch: 8 780 0.06187\n",
      "Train Epoch: 8 790 0.0124\n",
      "Train Epoch: 8 800 0.1268\n",
      "Train Epoch: 8 810 0.01313\n",
      "Train Epoch: 8 820 0.009394\n",
      "Train Epoch: 8 830 0.01058\n",
      "Train Epoch: 8 840 0.07513\n",
      "Train Epoch: 8 850 0.02728\n",
      "Train Epoch: 8 860 0.007408\n",
      "Train Epoch: 8 870 0.01061\n",
      "Train Epoch: 8 880 0.004882\n",
      "Train Epoch: 8 890 0.02918\n",
      "Train Epoch: 8 900 0.09202\n",
      "Train Epoch: 8 910 0.03658\n",
      "Train Epoch: 8 920 0.002791\n",
      "Train Epoch: 8 930 0.0004078\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.1%\n",
      "Epoch time: 178 seconds\n",
      "Epoch: 9\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 9 0 0.01517\n",
      "Train Epoch: 9 10 0.03481\n",
      "Train Epoch: 9 20 0.004749\n",
      "Train Epoch: 9 30 0.02542\n",
      "Train Epoch: 9 40 0.01096\n",
      "Train Epoch: 9 50 0.01969\n",
      "Train Epoch: 9 60 0.0005223\n",
      "Train Epoch: 9 70 0.0285\n",
      "Train Epoch: 9 80 0.01847\n",
      "Train Epoch: 9 90 0.007379\n",
      "Train Epoch: 9 100 0.2143\n",
      "Train Epoch: 9 110 0.09769\n",
      "Train Epoch: 9 120 0.01819\n",
      "Train Epoch: 9 130 0.04266\n",
      "Train Epoch: 9 140 0.1367\n",
      "Train Epoch: 9 150 0.02408\n",
      "Train Epoch: 9 160 0.07427\n",
      "Train Epoch: 9 170 0.004181\n",
      "Train Epoch: 9 180 0.03053\n",
      "Train Epoch: 9 190 0.06823\n",
      "Train Epoch: 9 200 0.0261\n",
      "Train Epoch: 9 210 0.003014\n",
      "Train Epoch: 9 220 0.0121\n",
      "Train Epoch: 9 230 0.02458\n",
      "Train Epoch: 9 240 0.01937\n",
      "Train Epoch: 9 250 0.0235\n",
      "Train Epoch: 9 260 0.02551\n",
      "Train Epoch: 9 270 0.003233\n",
      "Train Epoch: 9 280 0.04509\n",
      "Train Epoch: 9 290 0.03675\n",
      "Train Epoch: 9 300 0.02141\n",
      "Train Epoch: 9 310 0.01303\n",
      "Train Epoch: 9 320 0.003413\n",
      "Train Epoch: 9 330 0.02629\n",
      "Train Epoch: 9 340 0.002113\n",
      "Train Epoch: 9 350 0.01287\n",
      "Train Epoch: 9 360 0.01188\n",
      "Train Epoch: 9 370 0.04512\n",
      "Train Epoch: 9 380 0.0009303\n",
      "Train Epoch: 9 390 0.02949\n",
      "Train Epoch: 9 400 0.01172\n",
      "Train Epoch: 9 410 0.009643\n",
      "Train Epoch: 9 420 0.08998\n",
      "Train Epoch: 9 430 0.1037\n",
      "Train Epoch: 9 440 0.01456\n",
      "Train Epoch: 9 450 0.004594\n",
      "Train Epoch: 9 460 0.02809\n",
      "Train Epoch: 9 470 0.01165\n",
      "Train Epoch: 9 480 0.006554\n",
      "Train Epoch: 9 490 0.02269\n",
      "Train Epoch: 9 500 0.01362\n",
      "Train Epoch: 9 510 0.004582\n",
      "Train Epoch: 9 520 0.02689\n",
      "Train Epoch: 9 530 0.004922\n",
      "Train Epoch: 9 540 0.00294\n",
      "Train Epoch: 9 550 0.1585\n",
      "Train Epoch: 9 560 0.1435\n",
      "Train Epoch: 9 570 0.002587\n",
      "Train Epoch: 9 580 0.0425\n",
      "Train Epoch: 9 590 0.01234\n",
      "Train Epoch: 9 600 0.06835\n",
      "Train Epoch: 9 610 0.001171\n",
      "Train Epoch: 9 620 0.02733\n",
      "Train Epoch: 9 630 0.02259\n",
      "Train Epoch: 9 640 0.06976\n",
      "Train Epoch: 9 650 0.005546\n",
      "Train Epoch: 9 660 0.01634\n",
      "Train Epoch: 9 670 0.002565\n",
      "Train Epoch: 9 680 0.03207\n",
      "Train Epoch: 9 690 0.04125\n",
      "Train Epoch: 9 700 0.01844\n",
      "Train Epoch: 9 710 0.06401\n",
      "Train Epoch: 9 720 0.04701\n",
      "Train Epoch: 9 730 0.09256\n",
      "Train Epoch: 9 740 0.07687\n",
      "Train Epoch: 9 750 0.004539\n",
      "Train Epoch: 9 760 0.002646\n",
      "Train Epoch: 9 770 0.007965\n",
      "Train Epoch: 9 780 0.008493\n",
      "Train Epoch: 9 790 0.01217\n",
      "Train Epoch: 9 800 0.06075\n",
      "Train Epoch: 9 810 0.01463\n",
      "Train Epoch: 9 820 0.003217\n",
      "Train Epoch: 9 830 0.02468\n",
      "Train Epoch: 9 840 0.05383\n",
      "Train Epoch: 9 850 0.09805\n",
      "Train Epoch: 9 860 0.006737\n",
      "Train Epoch: 9 870 0.008677\n",
      "Train Epoch: 9 880 0.01459\n",
      "Train Epoch: 9 890 0.007756\n",
      "Train Epoch: 9 900 0.03249\n",
      "Train Epoch: 9 910 0.001081\n",
      "Train Epoch: 9 920 0.008137\n",
      "Train Epoch: 9 930 0.0002686\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.11999999999999%\n",
      "Epoch time: 175 seconds\n",
      "Epoch: 10\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 10 0 0.004171\n",
      "Train Epoch: 10 10 0.004015\n",
      "Train Epoch: 10 20 0.06953\n",
      "Train Epoch: 10 30 0.1215\n",
      "Train Epoch: 10 40 0.02471\n",
      "Train Epoch: 10 50 0.02082\n",
      "Train Epoch: 10 60 0.004157\n",
      "Train Epoch: 10 70 0.04374\n",
      "Train Epoch: 10 80 0.02313\n",
      "Train Epoch: 10 90 0.008318\n",
      "Train Epoch: 10 100 0.1217\n",
      "Train Epoch: 10 110 0.1381\n",
      "Train Epoch: 10 120 0.008175\n",
      "Train Epoch: 10 130 0.001992\n",
      "Train Epoch: 10 140 0.01249\n",
      "Train Epoch: 10 150 0.0106\n",
      "Train Epoch: 10 160 0.1218\n",
      "Train Epoch: 10 170 0.01203\n",
      "Train Epoch: 10 180 0.09054\n",
      "Train Epoch: 10 190 0.01196\n",
      "Train Epoch: 10 200 0.05372\n",
      "Train Epoch: 10 210 0.004314\n",
      "Train Epoch: 10 220 0.0286\n",
      "Train Epoch: 10 230 0.01602\n",
      "Train Epoch: 10 240 0.005498\n",
      "Train Epoch: 10 250 0.02742\n",
      "Train Epoch: 10 260 0.1242\n",
      "Train Epoch: 10 270 0.01575\n",
      "Train Epoch: 10 280 0.02991\n",
      "Train Epoch: 10 290 0.06496\n",
      "Train Epoch: 10 300 0.07569\n",
      "Train Epoch: 10 310 0.08366\n",
      "Train Epoch: 10 320 0.007859\n",
      "Train Epoch: 10 330 0.03357\n",
      "Train Epoch: 10 340 0.002986\n",
      "Train Epoch: 10 350 0.01014\n",
      "Train Epoch: 10 360 0.03058\n",
      "Train Epoch: 10 370 0.02149\n",
      "Train Epoch: 10 380 0.0005843\n",
      "Train Epoch: 10 390 0.005162\n",
      "Train Epoch: 10 400 0.00652\n",
      "Train Epoch: 10 410 0.005838\n",
      "Train Epoch: 10 420 0.03222\n",
      "Train Epoch: 10 430 0.1624\n",
      "Train Epoch: 10 440 0.01452\n",
      "Train Epoch: 10 450 0.03153\n",
      "Train Epoch: 10 460 0.01865\n",
      "Train Epoch: 10 470 0.0177\n",
      "Train Epoch: 10 480 0.005064\n",
      "Train Epoch: 10 490 0.02291\n",
      "Train Epoch: 10 500 0.02667\n",
      "Train Epoch: 10 510 0.002024\n",
      "Train Epoch: 10 520 0.01634\n",
      "Train Epoch: 10 530 0.008169\n",
      "Train Epoch: 10 540 0.01055\n",
      "Train Epoch: 10 550 0.1512\n",
      "Train Epoch: 10 560 0.07958\n",
      "Train Epoch: 10 570 0.004645\n",
      "Train Epoch: 10 580 0.01456\n",
      "Train Epoch: 10 590 0.01271\n",
      "Train Epoch: 10 600 0.00571\n",
      "Train Epoch: 10 610 0.006833\n",
      "Train Epoch: 10 620 0.01086\n",
      "Train Epoch: 10 630 0.009369\n",
      "Train Epoch: 10 640 0.0374\n",
      "Train Epoch: 10 650 0.01294\n",
      "Train Epoch: 10 660 0.01447\n",
      "Train Epoch: 10 670 0.005028\n",
      "Train Epoch: 10 680 0.05301\n",
      "Train Epoch: 10 690 0.003579\n",
      "Train Epoch: 10 700 0.02823\n",
      "Train Epoch: 10 710 0.009758\n",
      "Train Epoch: 10 720 0.01518\n",
      "Train Epoch: 10 730 0.09246\n",
      "Train Epoch: 10 740 0.05442\n",
      "Train Epoch: 10 750 0.00379\n",
      "Train Epoch: 10 760 0.01164\n",
      "Train Epoch: 10 770 0.02109\n",
      "Train Epoch: 10 780 0.04774\n",
      "Train Epoch: 10 790 0.03283\n",
      "Train Epoch: 10 800 0.09442\n",
      "Train Epoch: 10 810 0.04668\n",
      "Train Epoch: 10 820 0.002725\n",
      "Train Epoch: 10 830 0.02371\n",
      "Train Epoch: 10 840 0.07503\n",
      "Train Epoch: 10 850 0.02333\n",
      "Train Epoch: 10 860 0.01137\n",
      "Train Epoch: 10 870 0.05785\n",
      "Train Epoch: 10 880 0.01068\n",
      "Train Epoch: 10 890 0.01051\n",
      "Train Epoch: 10 900 0.07855\n",
      "Train Epoch: 10 910 0.0024\n",
      "Train Epoch: 10 920 0.0001599\n",
      "Train Epoch: 10 930 7.242e-05\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.13%\n",
      "Epoch time: 183 seconds\n",
      "Epoch: 11\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 11 0 0.009444\n",
      "Train Epoch: 11 10 0.001345\n",
      "Train Epoch: 11 20 0.00628\n",
      "Train Epoch: 11 30 0.003991\n",
      "Train Epoch: 11 40 0.02955\n",
      "Train Epoch: 11 50 0.009452\n",
      "Train Epoch: 11 60 0.0003161\n",
      "Train Epoch: 11 70 0.03107\n",
      "Train Epoch: 11 80 0.02812\n",
      "Train Epoch: 11 90 0.02656\n",
      "Train Epoch: 11 100 0.1195\n",
      "Train Epoch: 11 110 0.1883\n",
      "Train Epoch: 11 120 0.008182\n",
      "Train Epoch: 11 130 0.003296\n",
      "Train Epoch: 11 140 0.01764\n",
      "Train Epoch: 11 150 0.02336\n",
      "Train Epoch: 11 160 0.1215\n",
      "Train Epoch: 11 170 0.004686\n",
      "Train Epoch: 11 180 0.03331\n",
      "Train Epoch: 11 190 0.01862\n",
      "Train Epoch: 11 200 0.009028\n",
      "Train Epoch: 11 210 0.02064\n",
      "Train Epoch: 11 220 0.003117\n",
      "Train Epoch: 11 230 0.0139\n",
      "Train Epoch: 11 240 0.0308\n",
      "Train Epoch: 11 250 0.01706\n",
      "Train Epoch: 11 260 0.02565\n",
      "Train Epoch: 11 270 0.0006399\n",
      "Train Epoch: 11 280 0.01875\n",
      "Train Epoch: 11 290 0.009951\n",
      "Train Epoch: 11 300 0.01297\n",
      "Train Epoch: 11 310 0.06628\n",
      "Train Epoch: 11 320 0.001036\n",
      "Train Epoch: 11 330 0.07504\n",
      "Train Epoch: 11 340 0.002377\n",
      "Train Epoch: 11 350 0.09867\n",
      "Train Epoch: 11 360 0.01793\n",
      "Train Epoch: 11 370 0.01156\n",
      "Train Epoch: 11 380 0.0008989\n",
      "Train Epoch: 11 390 0.003406\n",
      "Train Epoch: 11 400 0.04442\n",
      "Train Epoch: 11 410 0.01618\n",
      "Train Epoch: 11 420 0.02286\n",
      "Train Epoch: 11 430 0.08643\n",
      "Train Epoch: 11 440 0.01267\n",
      "Train Epoch: 11 450 0.06097\n",
      "Train Epoch: 11 460 0.02521\n",
      "Train Epoch: 11 470 0.03241\n",
      "Train Epoch: 11 480 0.02539\n",
      "Train Epoch: 11 490 0.007901\n",
      "Train Epoch: 11 500 0.01046\n",
      "Train Epoch: 11 510 0.04207\n",
      "Train Epoch: 11 520 0.03686\n",
      "Train Epoch: 11 530 0.001735\n",
      "Train Epoch: 11 540 0.00215\n",
      "Train Epoch: 11 550 0.1115\n",
      "Train Epoch: 11 560 0.05665\n",
      "Train Epoch: 11 570 0.004659\n",
      "Train Epoch: 11 580 0.009417\n",
      "Train Epoch: 11 590 0.02618\n",
      "Train Epoch: 11 600 0.1761\n",
      "Train Epoch: 11 610 0.001087\n",
      "Train Epoch: 11 620 0.0405\n",
      "Train Epoch: 11 630 0.03892\n",
      "Train Epoch: 11 640 0.1129\n",
      "Train Epoch: 11 650 0.0327\n",
      "Train Epoch: 11 660 0.0009921\n",
      "Train Epoch: 11 670 0.01917\n",
      "Train Epoch: 11 680 0.05573\n",
      "Train Epoch: 11 690 0.00248\n",
      "Train Epoch: 11 700 0.006195\n",
      "Train Epoch: 11 710 0.03141\n",
      "Train Epoch: 11 720 0.03014\n",
      "Train Epoch: 11 730 0.09959\n",
      "Train Epoch: 11 740 0.0483\n",
      "Train Epoch: 11 750 0.01762\n",
      "Train Epoch: 11 760 0.01135\n",
      "Train Epoch: 11 770 0.01036\n",
      "Train Epoch: 11 780 0.0173\n",
      "Train Epoch: 11 790 0.006553\n",
      "Train Epoch: 11 800 0.09153\n",
      "Train Epoch: 11 810 0.005107\n",
      "Train Epoch: 11 820 0.002074\n",
      "Train Epoch: 11 830 0.05236\n",
      "Train Epoch: 11 840 0.06434\n",
      "Train Epoch: 11 850 0.04288\n",
      "Train Epoch: 11 860 0.01072\n",
      "Train Epoch: 11 870 0.01618\n",
      "Train Epoch: 11 880 0.01058\n",
      "Train Epoch: 11 890 0.007193\n",
      "Train Epoch: 11 900 0.1066\n",
      "Train Epoch: 11 910 0.001324\n",
      "Train Epoch: 11 920 0.00159\n",
      "Train Epoch: 11 930 0.005214\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.11%\n",
      "Epoch time: 165 seconds\n",
      "Epoch: 12\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 12 0 0.007299\n",
      "Train Epoch: 12 10 0.04357\n",
      "Train Epoch: 12 20 0.01955\n",
      "Train Epoch: 12 30 0.009897\n",
      "Train Epoch: 12 40 0.02951\n",
      "Train Epoch: 12 50 0.02281\n",
      "Train Epoch: 12 60 0.0008286\n",
      "Train Epoch: 12 70 0.02714\n",
      "Train Epoch: 12 80 0.05522\n",
      "Train Epoch: 12 90 0.01382\n",
      "Train Epoch: 12 100 0.1717\n",
      "Train Epoch: 12 110 0.1327\n",
      "Train Epoch: 12 120 0.003711\n",
      "Train Epoch: 12 130 0.0316\n",
      "Train Epoch: 12 140 0.01191\n",
      "Train Epoch: 12 150 0.007353\n",
      "Train Epoch: 12 160 0.1259\n",
      "Train Epoch: 12 170 0.006506\n",
      "Train Epoch: 12 180 0.02731\n",
      "Train Epoch: 12 190 0.02366\n",
      "Train Epoch: 12 200 0.03\n",
      "Train Epoch: 12 210 0.0004559\n",
      "Train Epoch: 12 220 0.005516\n",
      "Train Epoch: 12 230 0.01499\n",
      "Train Epoch: 12 240 0.009624\n",
      "Train Epoch: 12 250 0.03466\n",
      "Train Epoch: 12 260 0.04634\n",
      "Train Epoch: 12 270 0.003022\n",
      "Train Epoch: 12 280 0.01904\n",
      "Train Epoch: 12 290 0.01509\n",
      "Train Epoch: 12 300 0.1473\n",
      "Train Epoch: 12 310 0.07576\n",
      "Train Epoch: 12 320 0.01102\n",
      "Train Epoch: 12 330 0.02088\n",
      "Train Epoch: 12 340 0.003571\n",
      "Train Epoch: 12 350 0.003928\n",
      "Train Epoch: 12 360 0.0747\n",
      "Train Epoch: 12 370 0.02096\n",
      "Train Epoch: 12 380 0.02087\n",
      "Train Epoch: 12 390 0.007066\n",
      "Train Epoch: 12 400 0.004423\n",
      "Train Epoch: 12 410 0.007763\n",
      "Train Epoch: 12 420 0.04343\n",
      "Train Epoch: 12 430 0.1698\n",
      "Train Epoch: 12 440 0.09257\n",
      "Train Epoch: 12 450 0.01896\n",
      "Train Epoch: 12 460 0.02254\n",
      "Train Epoch: 12 470 0.02003\n",
      "Train Epoch: 12 480 0.06629\n",
      "Train Epoch: 12 490 0.02482\n",
      "Train Epoch: 12 500 0.03294\n",
      "Train Epoch: 12 510 0.004913\n",
      "Train Epoch: 12 520 0.02198\n",
      "Train Epoch: 12 530 0.004297\n",
      "Train Epoch: 12 540 0.02724\n",
      "Train Epoch: 12 550 0.1389\n",
      "Train Epoch: 12 560 0.02676\n",
      "Train Epoch: 12 570 0.01988\n",
      "Train Epoch: 12 580 0.006149\n",
      "Train Epoch: 12 590 0.0375\n",
      "Train Epoch: 12 600 0.05275\n",
      "Train Epoch: 12 610 0.007499\n",
      "Train Epoch: 12 620 0.06054\n",
      "Train Epoch: 12 630 0.01712\n",
      "Train Epoch: 12 640 0.02509\n",
      "Train Epoch: 12 650 0.01056\n",
      "Train Epoch: 12 660 0.02083\n",
      "Train Epoch: 12 670 0.005386\n",
      "Train Epoch: 12 680 0.0139\n",
      "Train Epoch: 12 690 0.006729\n",
      "Train Epoch: 12 700 0.04582\n",
      "Train Epoch: 12 710 0.03428\n",
      "Train Epoch: 12 720 0.02338\n",
      "Train Epoch: 12 730 0.1037\n",
      "Train Epoch: 12 740 0.02768\n",
      "Train Epoch: 12 750 0.005586\n",
      "Train Epoch: 12 760 0.0123\n",
      "Train Epoch: 12 770 0.002576\n",
      "Train Epoch: 12 780 0.01791\n",
      "Train Epoch: 12 790 0.002517\n",
      "Train Epoch: 12 800 0.0206\n",
      "Train Epoch: 12 810 0.006845\n",
      "Train Epoch: 12 820 0.001361\n",
      "Train Epoch: 12 830 0.003304\n",
      "Train Epoch: 12 840 0.1058\n",
      "Train Epoch: 12 850 0.0395\n",
      "Train Epoch: 12 860 0.002943\n",
      "Train Epoch: 12 870 0.03312\n",
      "Train Epoch: 12 880 0.002555\n",
      "Train Epoch: 12 890 0.003311\n",
      "Train Epoch: 12 900 0.02345\n",
      "Train Epoch: 12 910 0.006892\n",
      "Train Epoch: 12 920 0.001861\n",
      "Train Epoch: 12 930 0.0001622\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.11999999999999%\n",
      "Epoch time: 170 seconds\n",
      "Epoch: 13\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 13 0 0.001873\n",
      "Train Epoch: 13 10 0.006397\n",
      "Train Epoch: 13 20 0.01745\n",
      "Train Epoch: 13 30 0.0372\n",
      "Train Epoch: 13 40 0.00399\n",
      "Train Epoch: 13 50 0.001488\n",
      "Train Epoch: 13 60 0.001478\n",
      "Train Epoch: 13 70 0.00312\n",
      "Train Epoch: 13 80 0.09366\n",
      "Train Epoch: 13 90 0.005713\n",
      "Train Epoch: 13 100 0.1115\n",
      "Train Epoch: 13 110 0.1009\n",
      "Train Epoch: 13 120 0.01418\n",
      "Train Epoch: 13 130 0.01561\n",
      "Train Epoch: 13 140 0.004896\n",
      "Train Epoch: 13 150 0.003691\n",
      "Train Epoch: 13 160 0.07253\n",
      "Train Epoch: 13 170 0.01588\n",
      "Train Epoch: 13 180 0.06399\n",
      "Train Epoch: 13 190 0.02732\n",
      "Train Epoch: 13 200 0.03341\n",
      "Train Epoch: 13 210 0.03825\n",
      "Train Epoch: 13 220 0.003876\n",
      "Train Epoch: 13 230 0.06118\n",
      "Train Epoch: 13 240 0.01428\n",
      "Train Epoch: 13 250 0.01929\n",
      "Train Epoch: 13 260 0.05776\n",
      "Train Epoch: 13 270 0.006882\n",
      "Train Epoch: 13 280 0.01594\n",
      "Train Epoch: 13 290 0.002501\n",
      "Train Epoch: 13 300 0.1083\n",
      "Train Epoch: 13 310 0.06696\n",
      "Train Epoch: 13 320 0.00315\n",
      "Train Epoch: 13 330 0.02862\n",
      "Train Epoch: 13 340 0.01836\n",
      "Train Epoch: 13 350 0.008813\n",
      "Train Epoch: 13 360 0.004462\n",
      "Train Epoch: 13 370 0.03694\n",
      "Train Epoch: 13 380 0.001787\n",
      "Train Epoch: 13 390 0.004273\n",
      "Train Epoch: 13 400 0.004271\n",
      "Train Epoch: 13 410 0.03158\n",
      "Train Epoch: 13 420 0.02239\n",
      "Train Epoch: 13 430 0.05861\n",
      "Train Epoch: 13 440 0.03846\n",
      "Train Epoch: 13 450 0.01075\n",
      "Train Epoch: 13 460 0.01751\n",
      "Train Epoch: 13 470 0.01168\n",
      "Train Epoch: 13 480 0.03316\n",
      "Train Epoch: 13 490 0.006456\n",
      "Train Epoch: 13 500 0.0292\n",
      "Train Epoch: 13 510 0.00108\n",
      "Train Epoch: 13 520 0.02658\n",
      "Train Epoch: 13 530 0.005631\n",
      "Train Epoch: 13 540 0.005133\n",
      "Train Epoch: 13 550 0.05403\n",
      "Train Epoch: 13 560 0.06121\n",
      "Train Epoch: 13 570 0.003884\n",
      "Train Epoch: 13 580 0.01052\n",
      "Train Epoch: 13 590 0.1137\n",
      "Train Epoch: 13 600 0.074\n",
      "Train Epoch: 13 610 0.0001292\n",
      "Train Epoch: 13 620 0.03257\n",
      "Train Epoch: 13 630 0.0112\n",
      "Train Epoch: 13 640 0.01974\n",
      "Train Epoch: 13 650 0.007975\n",
      "Train Epoch: 13 660 0.006632\n",
      "Train Epoch: 13 670 0.01014\n",
      "Train Epoch: 13 680 0.03479\n",
      "Train Epoch: 13 690 0.004097\n",
      "Train Epoch: 13 700 0.008381\n",
      "Train Epoch: 13 710 0.06049\n",
      "Train Epoch: 13 720 0.06684\n",
      "Train Epoch: 13 730 0.1244\n",
      "Train Epoch: 13 740 0.06261\n",
      "Train Epoch: 13 750 0.01919\n",
      "Train Epoch: 13 760 0.02778\n",
      "Train Epoch: 13 770 0.004189\n",
      "Train Epoch: 13 780 0.01352\n",
      "Train Epoch: 13 790 0.008926\n",
      "Train Epoch: 13 800 0.04918\n",
      "Train Epoch: 13 810 0.05527\n",
      "Train Epoch: 13 820 0.004478\n",
      "Train Epoch: 13 830 0.02395\n",
      "Train Epoch: 13 840 0.09389\n",
      "Train Epoch: 13 850 0.06246\n",
      "Train Epoch: 13 860 0.001639\n",
      "Train Epoch: 13 870 0.03968\n",
      "Train Epoch: 13 880 0.02034\n",
      "Train Epoch: 13 890 0.003418\n",
      "Train Epoch: 13 900 0.0277\n",
      "Train Epoch: 13 910 0.01271\n",
      "Train Epoch: 13 920 0.0008133\n",
      "Train Epoch: 13 930 0.00143\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.11999999999999%\n",
      "Epoch time: 173 seconds\n",
      "Epoch: 14\n",
      "> Running train in 938 batches\n",
      "Train Epoch: 14 0 0.01184\n",
      "Train Epoch: 14 10 0.006226\n",
      "Train Epoch: 14 20 0.02982\n",
      "Train Epoch: 14 30 0.03476\n",
      "Train Epoch: 14 40 0.02294\n",
      "Train Epoch: 14 50 0.006038\n",
      "Train Epoch: 14 60 0.003408\n",
      "Train Epoch: 14 70 0.00869\n",
      "Train Epoch: 14 80 0.06008\n",
      "Train Epoch: 14 90 0.007195\n",
      "Train Epoch: 14 100 0.1137\n",
      "Train Epoch: 14 110 0.0421\n",
      "Train Epoch: 14 120 0.005352\n",
      "Train Epoch: 14 130 0.004972\n",
      "Train Epoch: 14 140 0.05566\n",
      "Train Epoch: 14 150 0.02457\n",
      "Train Epoch: 14 160 0.1347\n",
      "Train Epoch: 14 170 0.007541\n",
      "Train Epoch: 14 180 0.01876\n",
      "Train Epoch: 14 190 0.01845\n",
      "Train Epoch: 14 200 0.00857\n",
      "Train Epoch: 14 210 0.004862\n",
      "Train Epoch: 14 220 0.002799\n",
      "Train Epoch: 14 230 0.01563\n",
      "Train Epoch: 14 240 0.01495\n",
      "Train Epoch: 14 250 0.01059\n",
      "Train Epoch: 14 260 0.06543\n",
      "Train Epoch: 14 270 0.006405\n",
      "Train Epoch: 14 280 0.02489\n",
      "Train Epoch: 14 290 0.08482\n",
      "Train Epoch: 14 300 0.0511\n",
      "Train Epoch: 14 310 0.1416\n",
      "Train Epoch: 14 320 0.003778\n",
      "Train Epoch: 14 330 0.02799\n",
      "Train Epoch: 14 340 0.008924\n",
      "Train Epoch: 14 350 0.002488\n",
      "Train Epoch: 14 360 0.04465\n",
      "Train Epoch: 14 370 0.02201\n",
      "Train Epoch: 14 380 0.002526\n",
      "Train Epoch: 14 390 0.01144\n",
      "Train Epoch: 14 400 0.01998\n",
      "Train Epoch: 14 410 0.01171\n",
      "Train Epoch: 14 420 0.1509\n",
      "Train Epoch: 14 430 0.09395\n",
      "Train Epoch: 14 440 0.04367\n",
      "Train Epoch: 14 450 0.01203\n",
      "Train Epoch: 14 460 0.01105\n",
      "Train Epoch: 14 470 0.02413\n",
      "Train Epoch: 14 480 0.04873\n",
      "Train Epoch: 14 490 0.01813\n",
      "Train Epoch: 14 500 0.03724\n",
      "Train Epoch: 14 510 0.03349\n",
      "Train Epoch: 14 520 0.05327\n",
      "Train Epoch: 14 530 0.006694\n",
      "Train Epoch: 14 540 0.0006397\n",
      "Train Epoch: 14 550 0.04717\n",
      "Train Epoch: 14 560 0.114\n",
      "Train Epoch: 14 570 0.02949\n",
      "Train Epoch: 14 580 0.008007\n",
      "Train Epoch: 14 590 0.1331\n",
      "Train Epoch: 14 600 0.07711\n",
      "Train Epoch: 14 610 0.0009115\n",
      "Train Epoch: 14 620 0.05809\n",
      "Train Epoch: 14 630 0.01318\n",
      "Train Epoch: 14 640 0.007824\n",
      "Train Epoch: 14 650 0.01998\n",
      "Train Epoch: 14 660 0.006683\n",
      "Train Epoch: 14 670 0.02955\n",
      "Train Epoch: 14 680 0.01789\n",
      "Train Epoch: 14 690 0.004236\n",
      "Train Epoch: 14 700 0.02757\n",
      "Train Epoch: 14 710 0.03165\n",
      "Train Epoch: 14 720 0.03514\n",
      "Train Epoch: 14 730 0.07788\n",
      "Train Epoch: 14 740 0.04244\n",
      "Train Epoch: 14 750 0.01194\n",
      "Train Epoch: 14 760 0.01923\n",
      "Train Epoch: 14 770 0.00254\n",
      "Train Epoch: 14 780 0.01609\n",
      "Train Epoch: 14 790 0.03171\n",
      "Train Epoch: 14 800 0.1536\n",
      "Train Epoch: 14 810 0.007822\n",
      "Train Epoch: 14 820 0.001178\n",
      "Train Epoch: 14 830 0.007816\n",
      "Train Epoch: 14 840 0.03176\n",
      "Train Epoch: 14 850 0.07663\n",
      "Train Epoch: 14 860 0.003587\n",
      "Train Epoch: 14 870 0.02103\n",
      "Train Epoch: 14 880 0.004186\n",
      "Train Epoch: 14 890 0.006062\n",
      "Train Epoch: 14 900 0.0129\n",
      "Train Epoch: 14 910 0.0185\n",
      "Train Epoch: 14 920 0.01524\n",
      "Train Epoch: 14 930 0.0004816\n",
      "batch_idx >= train_batches, breaking\n",
      "> Running test_local in 10 batches\n",
      "batch_idx >= test_batches, breaking\n",
      "Test Set Accuracy: 99.13%\n",
      "Epoch time: 190 seconds\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "args[\"dry_run\"] = False  # comment to do a full train\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    # remote training on model with remote_torch\n",
    "    train(model, remote_torch, train_loader_ptr, optimizer, epoch, args, train_data_length)\n",
    "    # local testing on model with local torch\n",
    "    test_local(model, torch, test_loader, test_data_length)\n",
    "    scheduler.step()\n",
    "    epoch_end = time.time()\n",
    "    print(f\"Epoch time: {int(epoch_end - epoch_start)} seconds\")\n",
    "    if args[\"dry_run\"]:\n",
    "        break\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"save_model\"]:\n",
    "    model.get(\n",
    "        request_block=True,\n",
    "        reason=\"test evaluation\",\n",
    "        timeout_secs=5\n",
    "    ).save(\"./duet_mnist.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model would be no fun without the ability to do inference. The following code shows some examples on how we can do this either remotely or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_image_and_label(image, label):\n",
    "    fig = plt.figure()\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"none\")\n",
    "    plt.title(\"Ground Truth: {}\".format(label))\n",
    "    \n",
    "def prep_for_inference(image):\n",
    "    image_batch = image.unsqueeze(0).unsqueeze(0)\n",
    "    image_batch = image_batch * 1.0\n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_local(image, model):\n",
    "    if not model.is_local:\n",
    "        print(\"model is remote try .get()\")\n",
    "        return -1, torch.Tensor([-1])\n",
    "    image_tensor = torch.Tensor(prep_for_inference(image))\n",
    "    output = model(image_tensor)\n",
    "    preds = torch.exp(output)\n",
    "    local_y = preds\n",
    "    local_y = local_y.squeeze()\n",
    "    pos = local_y == max(local_y)\n",
    "    index = torch.nonzero(pos, as_tuple=False)\n",
    "    class_num = index.squeeze()\n",
    "    return class_num, local_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_remote(image, model):\n",
    "    if model.is_local:\n",
    "        print(\"model is local try .send()\")\n",
    "        return -1, remote_torch.Tensor([-1])\n",
    "    image_tensor_ptr = remote_torch.Tensor(prep_for_inference(image))\n",
    "    output = model(image_tensor_ptr)\n",
    "    preds = remote_torch.exp(output)\n",
    "    preds_result = preds.get(\n",
    "        request_block=True,\n",
    "        reason=\"To see a real world example of inference\",\n",
    "        timeout_secs=10\n",
    "    )\n",
    "    if preds_result is None:\n",
    "        print(\"No permission to do inference, request again\")\n",
    "        return -1, torch.Tensor([-1])\n",
    "    else:\n",
    "        # now we have the local tensor we can use local torch\n",
    "        local_y = torch.Tensor(preds_result)\n",
    "        local_y = local_y.squeeze()\n",
    "        pos = local_y == max(local_y)\n",
    "        index = torch.nonzero(pos, as_tuple=False)\n",
    "        class_num = index.squeeze()\n",
    "        return class_num, local_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Test Image: 6141\n",
      "Displaying 6141 == 141 in Batch: 6/10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets grab something from the test set\n",
    "import random\n",
    "total_images = test_data_length # 10000\n",
    "index = random.randint(0, total_images)\n",
    "print(\"Random Test Image:\", index)\n",
    "count = 0\n",
    "batch = index // test_kwargs[\"batch_size\"]\n",
    "batch_index = index % int(total_images / len(test_loader))\n",
    "for tensor_ptr in test_loader:\n",
    "    data, target = tensor_ptr[0], tensor_ptr[1]\n",
    "    if batch == count:\n",
    "        break\n",
    "    count += 1\n",
    "\n",
    "print(f\"Displaying {index} == {batch_index} in Batch: {batch}/{len(test_loader)}\")\n",
    "if batch_index > len(data):\n",
    "    batch_index = 0\n",
    "image_1 = data[batch_index].reshape((28, 28))\n",
    "label_1 = target[batch_index]\n",
    "draw_image_and_label(image_1, label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 Ground Truth: 1\n",
      "tensor([5.9786e-09, 9.9999e-01, 4.9142e-07, 7.7968e-09, 1.1567e-05, 1.1504e-06,\n",
      "        2.1769e-07, 3.9591e-07, 2.6034e-08, 7.4884e-08],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# classify remote\n",
    "class_num, preds = classify_remote(image_1, model)\n",
    "print(f\"Prediction: {class_num} Ground Truth: {label_1}\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = model.get(\n",
    "    request_block=True,\n",
    "    reason=\"To run test and inference locally\",\n",
    "    timeout_secs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 Ground Truth: 1\n",
      "tensor([7.5245e-10, 9.9999e-01, 1.1504e-07, 2.0754e-09, 1.6747e-06, 4.9939e-09,\n",
      "        3.2817e-06, 1.7546e-06, 3.3123e-07, 1.2196e-09],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# classify local\n",
    "class_num, preds = classify_local(image_1, local_model)\n",
    "print(f\"Prediction: {class_num} Ground Truth: {label_1}\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also download an image from the web and run inference on that\n",
    "from PIL import Image, ImageEnhance\n",
    "import PIL.ImageOps    \n",
    "\n",
    "import os\n",
    "def classify_url_image(image_url):\n",
    "    filename = os.path.basename(image_url)\n",
    "    os.system(f'curl -O {image_url}')\n",
    "    im = Image.open(filename)\n",
    "    im = PIL.ImageOps.invert(im)\n",
    "#     im = im.resize((28,28), Image.ANTIALIAS)\n",
    "    im = im.convert('LA')\n",
    "    enhancer = ImageEnhance.Brightness(im)\n",
    "    im = enhancer.enhance(3)\n",
    "\n",
    "\n",
    "    print(im.size)\n",
    "    fig = plt.figure()\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(im, cmap=\"gray\", interpolation=\"none\")\n",
    "    \n",
    "    # classify local\n",
    "    class_num, preds = classify_local(image_1, local_model)\n",
    "    print(f\"Prediction: {class_num}\")\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 93)\n",
      "Prediction: 1\n",
      "tensor([1.0391e-05, 9.9520e-01, 1.9898e-04, 1.2981e-05, 1.1125e-03, 6.7837e-04,\n",
      "        4.8594e-04, 1.7669e-04, 1.9784e-03, 1.4602e-04],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_url = \"https://raw.githubusercontent.com/kensanata/numbers/master/0018_CHXX/0/number-100.png\"\n",
    "classify_url_image(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
